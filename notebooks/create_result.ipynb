{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591afd66",
   "metadata": {},
   "source": [
    "# Script 1: prepare_data1_experiment_summary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160e7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Data1 written to: /mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/notebooks/store_output_cifar10_iter_ema_noise_validation_v2/data_to_report/data1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Prepare Data1 for reporting: aggregate experiment_summary.csv across runs/noise/alpha.\n",
    "\n",
    "Supported input folder layouts:\n",
    "\n",
    "A) Non-namespaced:\n",
    "  folder_store/\n",
    "    noise_0.2/alpha_0.2/experiment_summary.csv\n",
    "    noise_0.2/alpha_0.3/experiment_summary.csv\n",
    "    ...\n",
    "\n",
    "B) Namespaced by run_id:\n",
    "  folder_store/\n",
    "    run_2025_12_19/noise_0.2/alpha_0.2/experiment_summary.csv\n",
    "    run_2025_12_19/noise_0.2/alpha_0.3/experiment_summary.csv\n",
    "    ...\n",
    "\n",
    "Baseline definition:\n",
    "  Baseline for each (run_id, noise_ratio, alpha) is the row where iteration == 0.\n",
    "\n",
    "Selection protocol (matching your pipeline):\n",
    "  - Best iteration per (run_id, noise_ratio, alpha) selected by maximizing val_acc_noisy.\n",
    "  - Best alpha per (run_id, noise_ratio) selected by maximizing the best-iteration val_acc_noisy.\n",
    "\n",
    "Outputs written to:\n",
    "  folder_store/data_to_report/data1/\n",
    "    experiment_summary_all.csv\n",
    "    baseline_rows.csv\n",
    "    best_by_noisy_val_per_alpha.csv\n",
    "    alpha_sweep_ready.csv\n",
    "    best_alpha_per_noise.csv\n",
    "    main_table_ready.csv\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Settings:\n",
    "    folder_store: Path\n",
    "    out_dir: str = \"data_to_report/data1\"\n",
    "    overwrite: bool = True\n",
    "    selection_metric_col: str = \"val_acc_noisy\"\n",
    "    numeric_ratio_threshold: float = 0.95\n",
    "    enforce_path_meta: bool = False\n",
    "\n",
    "\n",
    "def _to_float_safe(x: str) -> Optional[float]:\n",
    "    try:\n",
    "        return float(str(x).replace(\",\", \".\"))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _read_csv_normalized(\n",
    "    path: Path,\n",
    "    force_str_cols: Optional[Set[str]] = None,\n",
    "    numeric_ratio_threshold: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV and normalize:\n",
    "      - replace comma decimals -> dot decimals for object cols\n",
    "      - convert columns to numeric when they look numeric-like\n",
    "      - keep non-numeric columns as strings\n",
    "\n",
    "    numeric_ratio_threshold:\n",
    "      if >= threshold fraction of non-empty values convert successfully -> treat as numeric.\n",
    "    \"\"\"\n",
    "    if force_str_cols is None:\n",
    "        force_str_cols = set()\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in force_str_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        s = df[col]\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            continue\n",
    "\n",
    "        # Normalize decimal commas for object columns\n",
    "        if pd.api.types.is_object_dtype(s):\n",
    "            s_str = s.astype(str).str.replace(\",\", \".\", regex=False)\n",
    "        else:\n",
    "            s_str = s.astype(str)\n",
    "\n",
    "        # Convert to numeric safely (no deprecated errors='ignore')\n",
    "        s_num = pd.to_numeric(s_str, errors=\"coerce\")\n",
    "\n",
    "        # Determine non-empty values to evaluate conversion ratio\n",
    "        s_str_norm = s_str.str.strip()\n",
    "        non_empty = s_str_norm.ne(\"\") & s_str_norm.str.lower().ne(\"nan\")\n",
    "        denom = int(non_empty.sum())\n",
    "\n",
    "        if denom == 0:\n",
    "            df[col] = s_str\n",
    "            continue\n",
    "\n",
    "        ok = int(s_num[non_empty].notna().sum())\n",
    "        if ok / denom >= numeric_ratio_threshold:\n",
    "            df[col] = s_num\n",
    "        else:\n",
    "            df[col] = s_str\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _find_experiment_summaries(folder_store: Path) -> List[Tuple[Optional[str], Path]]:\n",
    "    \"\"\"\n",
    "    Return list of (run_id, csv_path) for experiment_summary.csv.\n",
    "    Supports both layouts:\n",
    "      - folder_store/noise_*/alpha_*/experiment_summary.csv\n",
    "      - folder_store/<run_id>/noise_*/alpha_*/experiment_summary.csv\n",
    "    \"\"\"\n",
    "    folder_store = folder_store.resolve()\n",
    "\n",
    "    direct = list(folder_store.glob(\"noise_*/alpha_*/experiment_summary.csv\"))\n",
    "    if direct:\n",
    "        return [(None, p) for p in direct]\n",
    "\n",
    "    results: List[Tuple[Optional[str], Path]] = []\n",
    "    for run_dir in folder_store.iterdir():\n",
    "        if not run_dir.is_dir():\n",
    "            continue\n",
    "        for p in run_dir.glob(\"noise_*/alpha_*/experiment_summary.csv\"):\n",
    "            results.append((run_dir.name, p))\n",
    "    return results\n",
    "\n",
    "\n",
    "def _extract_noise_alpha_from_path(csv_path: Path) -> Tuple[Optional[float], Optional[float]]:\n",
    "    noise_val: Optional[float] = None\n",
    "    alpha_val: Optional[float] = None\n",
    "\n",
    "    for parent in csv_path.parents:\n",
    "        name = parent.name\n",
    "        if name.startswith(\"noise_\"):\n",
    "            noise_val = _to_float_safe(name.replace(\"noise_\", \"\"))\n",
    "        elif name.startswith(\"alpha_\"):\n",
    "            alpha_val = _to_float_safe(name.replace(\"alpha_\", \"\"))\n",
    "\n",
    "    return noise_val, alpha_val\n",
    "\n",
    "\n",
    "def _ensure_meta_columns(df: pd.DataFrame, csv_path: Path, enforce_path_meta: bool) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    noise_from_path, alpha_from_path = _extract_noise_alpha_from_path(csv_path)\n",
    "\n",
    "    if enforce_path_meta:\n",
    "        if noise_from_path is not None:\n",
    "            df[\"noise_ratio\"] = noise_from_path\n",
    "        if alpha_from_path is not None:\n",
    "            df[\"alpha\"] = alpha_from_path\n",
    "        return df\n",
    "\n",
    "    if \"noise_ratio\" not in df.columns and noise_from_path is not None:\n",
    "        df[\"noise_ratio\"] = noise_from_path\n",
    "    if \"alpha\" not in df.columns and alpha_from_path is not None:\n",
    "        df[\"alpha\"] = alpha_from_path\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _pick_best_row_per_group(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: List[str],\n",
    "    metric_col: str,\n",
    "    tie_test_col: str = \"test_acc_reported\",\n",
    "    tie_kept_col: str = \"kept_ratio\",\n",
    "    iter_col: str = \"iteration\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pick best row per group by:\n",
    "      1) metric_col descending\n",
    "      2) tie_test_col descending\n",
    "      3) tie_kept_col descending\n",
    "      4) iteration ascending\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "\n",
    "    for c in [metric_col, tie_test_col, tie_kept_col, iter_col]:\n",
    "        if c not in df2.columns:\n",
    "            df2[c] = pd.NA\n",
    "\n",
    "    df2 = df2.sort_values(\n",
    "        by=[metric_col, tie_test_col, tie_kept_col, iter_col],\n",
    "        ascending=[False, False, False, True],\n",
    "    )\n",
    "\n",
    "    best = df2.groupby(group_cols, as_index=False).head(1).copy()\n",
    "    return best\n",
    "\n",
    "\n",
    "def build_data1(settings: Settings) -> Path:\n",
    "    folder_store = settings.folder_store.resolve()\n",
    "    out_dir = folder_store / settings.out_dir\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    entries = _find_experiment_summaries(folder_store)\n",
    "    if not entries:\n",
    "        raise FileNotFoundError(\n",
    "            \"No experiment_summary.csv found. Expected noise_*/alpha_*/experiment_summary.csv \"\n",
    "            \"under folder_store (or under one extra run_id folder).\"\n",
    "        )\n",
    "\n",
    "    all_frames: List[pd.DataFrame] = []\n",
    "    for run_id, csv_path in entries:\n",
    "        df = _read_csv_normalized(\n",
    "            csv_path,\n",
    "            force_str_cols={\"timestamp\"},\n",
    "            numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "        )\n",
    "        df = _ensure_meta_columns(df, csv_path, settings.enforce_path_meta)\n",
    "\n",
    "        if \"iteration\" not in df.columns:\n",
    "            raise ValueError(f\"Missing 'iteration' column in: {csv_path}\")\n",
    "        if \"noise_ratio\" not in df.columns or \"alpha\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing 'noise_ratio' or 'alpha' in: {csv_path}. \"\n",
    "                \"Either include them in CSV or ensure folder name noise_x/alpha_y.\"\n",
    "            )\n",
    "\n",
    "        df[\"run_id\"] = run_id if run_id is not None else \"default\"\n",
    "        df[\"source_path\"] = str(csv_path)\n",
    "        all_frames.append(df)\n",
    "\n",
    "    df_all = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    metric_col = settings.selection_metric_col\n",
    "    if metric_col not in df_all.columns:\n",
    "        raise ValueError(\n",
    "            f\"Selection metric column '{metric_col}' not found. \"\n",
    "            f\"Available columns: {list(df_all.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Baseline rows\n",
    "    df_baseline = df_all[df_all[\"iteration\"] == 0].copy()\n",
    "\n",
    "    # Best per (run_id, noise, alpha)\n",
    "    group_cols_alpha = [\"run_id\", \"noise_ratio\", \"alpha\"]\n",
    "    df_best_per_alpha = _pick_best_row_per_group(df_all, group_cols_alpha, metric_col)\n",
    "    df_best_per_alpha = df_best_per_alpha.rename(columns={\"iteration\": \"best_iteration\"})\n",
    "\n",
    "    # Alpha sweep ready (thin)\n",
    "    alpha_cols = [\n",
    "        \"run_id\",\n",
    "        \"noise_ratio\",\n",
    "        \"alpha\",\n",
    "        \"best_iteration\",\n",
    "        metric_col,\n",
    "        \"val_acc_reported\",\n",
    "        \"val_acc_orig\",\n",
    "        \"test_acc_reported\",\n",
    "        \"test_acc_orig\",\n",
    "        \"kept_ratio\",\n",
    "        \"samples_kept\",\n",
    "        \"samples_removed\",\n",
    "        \"samples_total\",\n",
    "        \"training_samples_used\",\n",
    "        \"timestamp\",\n",
    "        \"source_path\",\n",
    "    ]\n",
    "    alpha_cols = [c for c in alpha_cols if c in df_best_per_alpha.columns]\n",
    "    df_alpha_sweep = df_best_per_alpha[alpha_cols].copy()\n",
    "\n",
    "    # Best alpha per (run_id, noise)\n",
    "    group_cols_noise = [\"run_id\", \"noise_ratio\"]\n",
    "    # Use df_best_per_alpha rows (one per alpha), pick best alpha by metric_col\n",
    "    df_best_alpha = _pick_best_row_per_group(df_best_per_alpha, group_cols_noise, metric_col)\n",
    "    df_best_alpha = df_best_alpha.rename(columns={\"alpha\": \"best_alpha\"})\n",
    "\n",
    "    # Main table ready: baseline vs ours per (run_id, noise)\n",
    "    # Ours row:\n",
    "    df_ours = df_best_alpha.copy()\n",
    "    df_ours[\"Method\"] = \"Ours\"\n",
    "    df_ours[\"alpha\"] = df_ours[\"best_alpha\"]  # unify naming\n",
    "\n",
    "    # Baseline row: try baseline for that best_alpha; otherwise fallback any baseline for noise.\n",
    "    base_keys = [\"run_id\", \"noise_ratio\", \"alpha\"]\n",
    "    df_base = df_best_alpha[[\"run_id\", \"noise_ratio\", \"best_alpha\"]].copy()\n",
    "    df_base = df_base.rename(columns={\"best_alpha\": \"alpha\"})\n",
    "\n",
    "    df_base = df_base.merge(\n",
    "        df_baseline,\n",
    "        how=\"left\",\n",
    "        on=base_keys,\n",
    "        suffixes=(\"\", \"_baseline\"),\n",
    "    )\n",
    "\n",
    "    # Fill missing baseline rows with fallback baseline (any alpha) within same (run_id, noise_ratio)\n",
    "    missing = df_base[\"iteration\"].isna()\n",
    "    if missing.any():\n",
    "        fallback = (\n",
    "            df_baseline.sort_values([\"run_id\", \"noise_ratio\", \"alpha\"])\n",
    "            .groupby([\"run_id\", \"noise_ratio\"], as_index=False)\n",
    "            .head(1)\n",
    "        )\n",
    "        df_base_missing = df_base.loc[missing, [\"run_id\", \"noise_ratio\"]].merge(\n",
    "            fallback,\n",
    "            on=[\"run_id\", \"noise_ratio\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        df_base.loc[missing, df_base_missing.columns] = df_base_missing.values\n",
    "\n",
    "    df_base[\"Method\"] = \"Baseline\"\n",
    "    # For baseline: best_iteration concept is 0\n",
    "    df_base[\"best_iteration\"] = 0\n",
    "\n",
    "    # Columns for main table (only keep what exists)\n",
    "    main_cols = [\n",
    "        \"run_id\",\n",
    "        \"noise_ratio\",\n",
    "        \"Method\",\n",
    "        \"alpha\",\n",
    "        \"best_iteration\",\n",
    "        metric_col,\n",
    "        \"val_acc_reported\",\n",
    "        \"val_acc_orig\",\n",
    "        \"test_acc_reported\",\n",
    "        \"test_acc_orig\",\n",
    "        \"kept_ratio\",\n",
    "        \"samples_kept\",\n",
    "        \"samples_removed\",\n",
    "        \"samples_total\",\n",
    "        \"training_samples_used\",\n",
    "    ]\n",
    "    main_cols = [c for c in main_cols if c in df_all.columns or c in df_ours.columns or c in df_base.columns]\n",
    "\n",
    "    df_main = pd.concat(\n",
    "        [\n",
    "            df_base.reindex(columns=main_cols, fill_value=pd.NA),\n",
    "            df_ours.reindex(columns=main_cols, fill_value=pd.NA),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    ).sort_values([\"run_id\", \"noise_ratio\", \"Method\"])\n",
    "\n",
    "    # Write outputs\n",
    "    outputs = {\n",
    "        \"experiment_summary_all.csv\": df_all,\n",
    "        \"baseline_rows.csv\": df_baseline,\n",
    "        \"best_by_noisy_val_per_alpha.csv\": df_best_per_alpha,\n",
    "        \"alpha_sweep_ready.csv\": df_alpha_sweep,\n",
    "        \"best_alpha_per_noise.csv\": df_best_alpha,\n",
    "        \"main_table_ready.csv\": df_main,\n",
    "    }\n",
    "\n",
    "    for fname, df in outputs.items():\n",
    "        out_path = out_dir / fname\n",
    "        if settings.overwrite or not out_path.exists():\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====== INPUT SETTINGS (edit here) ======\n",
    "    folder_store = \"store_output_cifar10_iter_ema_noise_validation_v2\"\n",
    "    # =======================================\n",
    "\n",
    "    out = build_data1(Settings(folder_store=Path(folder_store)))\n",
    "    print(f\"[OK] Data1 written to: {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cd305",
   "metadata": {},
   "source": [
    "# Script 2: prepare_data2_filter_quality.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a56474-04b9-4fdb-a011-468519c3badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Data2 written to: /mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/notebooks/store_output_cifar10_iter_ema_noise_validation_v2/data_to_report/data2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Prepare Data2 for reporting: compute filter quality from train_kept_*.csv files.\n",
    "\n",
    "Supported input folder layouts:\n",
    "\n",
    "A) Non-namespaced:\n",
    "  folder_store/noise_0.2/alpha_0.2/iteration_0/train_kept_0.csv\n",
    "\n",
    "B) Namespaced:\n",
    "  folder_store/<run_id>/noise_0.2/alpha_0.2/iteration_0/train_kept_0.csv\n",
    "\n",
    "train_kept_*.csv contains ONLY kept samples.\n",
    "We compute:\n",
    "  - precision_kept (exact): clean_kept_count / kept_count\n",
    "  - recall_clean_total using:\n",
    "        clean_total = round((1 - noise_ratio) * samples_total)\n",
    "    where samples_total is read from experiment_summary.csv if available,\n",
    "    else defaults to 45000.\n",
    "\n",
    "Outputs written to:\n",
    "  folder_store/data_to_report/data2/\n",
    "    filter_quality_all.csv\n",
    "    class_balance_long.csv\n",
    "    class_balance_clean_noisy_long.csv\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Settings:\n",
    "    folder_store: Path\n",
    "    out_dir: str = \"data_to_report/data2\"\n",
    "    default_samples_total: int = 45000\n",
    "    overwrite: bool = True\n",
    "    warn_iteration_mismatch: bool = True\n",
    "    numeric_ratio_threshold: float = 0.95\n",
    "\n",
    "\n",
    "def _to_float_safe(x: str) -> Optional[float]:\n",
    "    try:\n",
    "        return float(str(x).replace(\",\", \".\"))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _read_csv_normalized(\n",
    "    path: Path,\n",
    "    force_str_cols: Optional[Set[str]] = None,\n",
    "    numeric_ratio_threshold: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    if force_str_cols is None:\n",
    "        force_str_cols = set()\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in force_str_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        s = df[col]\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_object_dtype(s):\n",
    "            s_str = s.astype(str).str.replace(\",\", \".\", regex=False)\n",
    "        else:\n",
    "            s_str = s.astype(str)\n",
    "\n",
    "        s_num = pd.to_numeric(s_str, errors=\"coerce\")\n",
    "\n",
    "        s_str_norm = s_str.str.strip()\n",
    "        non_empty = s_str_norm.ne(\"\") & s_str_norm.str.lower().ne(\"nan\")\n",
    "        denom = int(non_empty.sum())\n",
    "\n",
    "        if denom == 0:\n",
    "            df[col] = s_str\n",
    "            continue\n",
    "\n",
    "        ok = int(s_num[non_empty].notna().sum())\n",
    "        if ok / denom >= numeric_ratio_threshold:\n",
    "            df[col] = s_num\n",
    "        else:\n",
    "            df[col] = s_str\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _find_kept_files(folder_store: Path) -> List[Tuple[Optional[str], Path]]:\n",
    "    folder_store = folder_store.resolve()\n",
    "\n",
    "    direct = list(folder_store.glob(\"noise_*/alpha_*/iteration_*/train_kept_*.csv\"))\n",
    "    if direct:\n",
    "        return [(None, p) for p in direct]\n",
    "\n",
    "    results: List[Tuple[Optional[str], Path]] = []\n",
    "    for run_dir in folder_store.iterdir():\n",
    "        if not run_dir.is_dir():\n",
    "            continue\n",
    "        for p in run_dir.glob(\"noise_*/alpha_*/iteration_*/train_kept_*.csv\"):\n",
    "            results.append((run_dir.name, p))\n",
    "    return results\n",
    "\n",
    "\n",
    "def _extract_meta_from_path(p: Path) -> Tuple[Optional[float], Optional[float], Optional[int], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extract:\n",
    "      noise_ratio, alpha, iteration_from_folder, iteration_from_filename\n",
    "    From:\n",
    "      .../noise_0.2/alpha_0.6/iteration_4/train_kept_4.csv\n",
    "    \"\"\"\n",
    "    noise_val: Optional[float] = None\n",
    "    alpha_val: Optional[float] = None\n",
    "    iter_folder: Optional[int] = None\n",
    "\n",
    "    for part in p.parts:\n",
    "        if part.startswith(\"noise_\"):\n",
    "            noise_val = _to_float_safe(part.replace(\"noise_\", \"\"))\n",
    "        elif part.startswith(\"alpha_\"):\n",
    "            alpha_val = _to_float_safe(part.replace(\"alpha_\", \"\"))\n",
    "        elif part.startswith(\"iteration_\"):\n",
    "            try:\n",
    "                iter_folder = int(part.replace(\"iteration_\", \"\"))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    iter_file: Optional[int] = None\n",
    "    name = p.name\n",
    "    if name.startswith(\"train_kept_\") and name.endswith(\".csv\"):\n",
    "        raw = name.replace(\"train_kept_\", \"\").replace(\".csv\", \"\")\n",
    "        try:\n",
    "            iter_file = int(raw)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return noise_val, alpha_val, iter_folder, iter_file\n",
    "\n",
    "\n",
    "def _find_experiment_summary(folder_store: Path, run_id: Optional[str], noise: float, alpha: float) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Locate experiment_summary.csv for the same (noise, alpha) under same run_id.\n",
    "    Robust to float formatting in folder names.\n",
    "    \"\"\"\n",
    "    root = folder_store / run_id if run_id else folder_store\n",
    "\n",
    "    # Fast path: exact string\n",
    "    p = root / f\"noise_{noise}\" / f\"alpha_{alpha}\" / \"experiment_summary.csv\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    # Robust scan\n",
    "    noise_dir = None\n",
    "    for d in root.glob(\"noise_*\"):\n",
    "        val = _to_float_safe(d.name.replace(\"noise_\", \"\"))\n",
    "        if val is not None and abs(val - noise) < 1e-9:\n",
    "            noise_dir = d\n",
    "            break\n",
    "    if noise_dir is None:\n",
    "        return None\n",
    "\n",
    "    alpha_dir = None\n",
    "    for d in noise_dir.glob(\"alpha_*\"):\n",
    "        val = _to_float_safe(d.name.replace(\"alpha_\", \"\"))\n",
    "        if val is not None and abs(val - alpha) < 1e-9:\n",
    "            alpha_dir = d\n",
    "            break\n",
    "    if alpha_dir is None:\n",
    "        return None\n",
    "\n",
    "    p2 = alpha_dir / \"experiment_summary.csv\"\n",
    "    return p2 if p2.exists() else None\n",
    "\n",
    "\n",
    "def build_data2(settings: Settings) -> Path:\n",
    "    folder_store = settings.folder_store.resolve()\n",
    "    out_dir = folder_store / settings.out_dir\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    kept_files = _find_kept_files(folder_store)\n",
    "    if not kept_files:\n",
    "        raise FileNotFoundError(\n",
    "            \"No train_kept_*.csv found. Expected noise_*/alpha_*/iteration_*/train_kept_*.csv.\"\n",
    "        )\n",
    "\n",
    "    quality_rows: List[Dict] = []\n",
    "    class_long_rows: List[Dict] = []\n",
    "    class_cn_rows: List[Dict] = []\n",
    "\n",
    "    for run_id, csv_path in kept_files:\n",
    "        noise, alpha, iter_folder, iter_file = _extract_meta_from_path(csv_path)\n",
    "        if noise is None or alpha is None or iter_folder is None:\n",
    "            continue\n",
    "\n",
    "        if settings.warn_iteration_mismatch and iter_file is not None and iter_file != iter_folder:\n",
    "            print(\n",
    "                f\"[WARN] Iteration mismatch: folder iteration_{iter_folder} but filename train_kept_{iter_file}.csv \"\n",
    "                f\"({csv_path})\"\n",
    "            )\n",
    "\n",
    "        df = _read_csv_normalized(\n",
    "            csv_path,\n",
    "            force_str_cols={\"image_path\", \"class_name\", \"split\", \"filter_flag\"},\n",
    "            numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "        )\n",
    "\n",
    "        # Clean mask\n",
    "        if \"noise_flag\" in df.columns:\n",
    "            clean_mask = df[\"noise_flag\"] == 0\n",
    "        elif \"label_noisy\" in df.columns and \"label_orig\" in df.columns:\n",
    "            clean_mask = df[\"label_noisy\"] == df[\"label_orig\"]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Missing columns to compute clean mask in {csv_path}. \"\n",
    "                \"Need noise_flag OR (label_noisy,label_orig).\"\n",
    "            )\n",
    "\n",
    "        kept_count = int(len(df))\n",
    "        clean_kept = int(clean_mask.sum())\n",
    "        noisy_kept = int((~clean_mask).sum())\n",
    "\n",
    "        precision = clean_kept / kept_count if kept_count > 0 else 0.0\n",
    "        noisy_kept_rate = noisy_kept / kept_count if kept_count > 0 else 0.0\n",
    "\n",
    "        # samples_total\n",
    "        samples_total = settings.default_samples_total\n",
    "        sum_path = _find_experiment_summary(folder_store, run_id, noise, alpha)\n",
    "        if sum_path is not None:\n",
    "            df_sum = _read_csv_normalized(\n",
    "                sum_path,\n",
    "                force_str_cols={\"timestamp\"},\n",
    "                numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "            )\n",
    "            if \"samples_total\" in df_sum.columns and len(df_sum) > 0:\n",
    "                try:\n",
    "                    samples_total = int(df_sum[\"samples_total\"].iloc[0])\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        clean_total = max(int(round((1.0 - noise) * samples_total)), 1)\n",
    "        recall = clean_kept / clean_total\n",
    "        f1 = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "\n",
    "        kept_ratio = kept_count / samples_total\n",
    "        clean_kept_ratio_of_total = clean_kept / samples_total\n",
    "\n",
    "        quality_rows.append(\n",
    "            dict(\n",
    "                run_id=run_id if run_id else \"default\",\n",
    "                noise_ratio=noise,\n",
    "                alpha=alpha,\n",
    "                iteration=iter_folder,\n",
    "                samples_total=samples_total,\n",
    "                clean_total=clean_total,\n",
    "                kept_count=kept_count,\n",
    "                clean_kept_count=clean_kept,\n",
    "                noisy_kept_count=noisy_kept,\n",
    "                kept_ratio=kept_ratio,\n",
    "                clean_kept_ratio_of_total=clean_kept_ratio_of_total,\n",
    "                precision_kept=precision,\n",
    "                recall_clean_total=recall,\n",
    "                f1_clean_total=f1,\n",
    "                noisy_kept_rate=noisy_kept_rate,\n",
    "                source_path=str(csv_path),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Class balance (long format)\n",
    "        class_col: Optional[str] = None\n",
    "        if \"class_name\" in df.columns:\n",
    "            class_col = \"class_name\"\n",
    "        elif \"label_orig\" in df.columns:\n",
    "            class_col = \"label_orig\"\n",
    "\n",
    "        if class_col is not None:\n",
    "            vc_total = df[class_col].value_counts()\n",
    "            for cls, cnt in vc_total.items():\n",
    "                class_long_rows.append(\n",
    "                    dict(\n",
    "                        run_id=run_id if run_id else \"default\",\n",
    "                        noise_ratio=noise,\n",
    "                        alpha=alpha,\n",
    "                        iteration=iter_folder,\n",
    "                        class_name=str(cls),\n",
    "                        kept_count=int(cnt),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            df_clean = df[clean_mask]\n",
    "            df_noisy = df[~clean_mask]\n",
    "            vc_clean = df_clean[class_col].value_counts()\n",
    "            vc_noisy = df_noisy[class_col].value_counts()\n",
    "            all_classes = set(vc_total.index.tolist()) | set(vc_clean.index.tolist()) | set(vc_noisy.index.tolist())\n",
    "\n",
    "            for cls in sorted(all_classes, key=lambda x: str(x)):\n",
    "                class_cn_rows.append(\n",
    "                    dict(\n",
    "                        run_id=run_id if run_id else \"default\",\n",
    "                        noise_ratio=noise,\n",
    "                        alpha=alpha,\n",
    "                        iteration=iter_folder,\n",
    "                        class_name=str(cls),\n",
    "                        kept_count_total=int(vc_total.get(cls, 0)),\n",
    "                        kept_count_clean=int(vc_clean.get(cls, 0)),\n",
    "                        kept_count_noisy=int(vc_noisy.get(cls, 0)),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    df_quality = pd.DataFrame(quality_rows).sort_values([\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\"])\n",
    "    df_class_long = pd.DataFrame(class_long_rows).sort_values(\n",
    "        [\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", \"class_name\"]\n",
    "    )\n",
    "    df_class_cn = pd.DataFrame(class_cn_rows).sort_values(\n",
    "        [\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", \"class_name\"]\n",
    "    )\n",
    "\n",
    "    p_quality = out_dir / \"filter_quality_all.csv\"\n",
    "    p_class_long = out_dir / \"class_balance_long.csv\"\n",
    "    p_class_cn = out_dir / \"class_balance_clean_noisy_long.csv\"\n",
    "\n",
    "    if settings.overwrite or not p_quality.exists():\n",
    "        df_quality.to_csv(p_quality, index=False)\n",
    "    if settings.overwrite or not p_class_long.exists():\n",
    "        df_class_long.to_csv(p_class_long, index=False)\n",
    "    if settings.overwrite or not p_class_cn.exists():\n",
    "        df_class_cn.to_csv(p_class_cn, index=False)\n",
    "\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====== INPUT SETTINGS (edit here) ======\n",
    "    folder_store = \"store_output_cifar10_iter_ema_noise_validation_v2\"\n",
    "    # =======================================\n",
    "\n",
    "    out = build_data2(Settings(folder_store=Path(folder_store)))\n",
    "    print(f\"[OK] Data2 written to: {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67396cb5",
   "metadata": {},
   "source": [
    "# 3 generate_report_assets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f94ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Report assets written to: /mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/notebooks/store_output_cifar10_iter_ema_noise_validation_v2/report_assets\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "generate_report_assets.py (UPGRADED)\n",
    "\n",
    "- Generate tables/figures for report from prepared Data1/Data2\n",
    "- Add Table 3: filter_summary_table.csv (Precision/Recall/F1 at selected best alpha+iteration)\n",
    "- Create captions.md with Table 1–3 and figure captions\n",
    "- Minimal annotation: highlight only:\n",
    "    ★ Selected (by noisy-val rule)\n",
    "    ▲ Oracle best (by test_acc) for analysis\n",
    "\n",
    "Prerequisites:\n",
    "folder_store/\n",
    "  data_to_report/\n",
    "    data1/\n",
    "      experiment_summary_all.csv\n",
    "      alpha_sweep_ready.csv\n",
    "      best_alpha_per_noise.csv\n",
    "      main_table_ready.csv\n",
    "    data2/\n",
    "      filter_quality_all.csv\n",
    "      class_balance_long.csv (optional)\n",
    "      class_balance_clean_noisy_long.csv (optional)\n",
    "\n",
    "Outputs:\n",
    "folder_store/report_assets/\n",
    "  tables/\n",
    "    main_table_ready.csv\n",
    "    best_alpha_per_noise.csv\n",
    "    filter_summary_table.csv   <-- NEW\n",
    "  figures/\n",
    "    ...\n",
    "  captions.md\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Robust CSV normalization\n",
    "# -------------------------\n",
    "def read_csv_normalized(\n",
    "    path: Path,\n",
    "    force_str_cols: Optional[Set[str]] = None,\n",
    "    numeric_ratio_threshold: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV and normalize:\n",
    "      - replace comma decimals -> dot decimals for object cols\n",
    "      - convert numeric-like columns to numeric (>= threshold)\n",
    "    \"\"\"\n",
    "    if force_str_cols is None:\n",
    "        force_str_cols = set()\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in force_str_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        s = df[col]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_object_dtype(s):\n",
    "            s_str = s.astype(str).str.replace(\",\", \".\", regex=False)\n",
    "        else:\n",
    "            s_str = s.astype(str)\n",
    "\n",
    "        s_num = pd.to_numeric(s_str, errors=\"coerce\")\n",
    "\n",
    "        s_norm = s_str.str.strip()\n",
    "        non_empty = s_norm.ne(\"\") & s_norm.str.lower().ne(\"nan\")\n",
    "        denom = int(non_empty.sum())\n",
    "        if denom == 0:\n",
    "            df[col] = s_str\n",
    "            continue\n",
    "\n",
    "        ok = int(s_num[non_empty].notna().sum())\n",
    "        if ok / denom >= numeric_ratio_threshold:\n",
    "            df[col] = s_num\n",
    "        else:\n",
    "            df[col] = s_str\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "@dataclass(frozen=True)\n",
    "class Settings:\n",
    "    folder_store: Path\n",
    "    out_dir: str = \"report_assets\"\n",
    "    dpi: int = 300\n",
    "    numeric_ratio_threshold: float = 0.95\n",
    "\n",
    "    # columns\n",
    "    test_acc_col: str = \"test_acc_reported\"\n",
    "    val_noisy_col: str = \"val_acc_noisy\"\n",
    "    val_orig_col: str = \"val_acc_orig\"\n",
    "    kept_ratio_col: str = \"kept_ratio\"\n",
    "\n",
    "    # annotation\n",
    "    ann_decimals: int = 4\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def ensure_exists(path: Path, desc: str) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {desc}: {path}\")\n",
    "\n",
    "\n",
    "def savefig(path: Path, dpi: int) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def expected_clean_ratio(noise_ratio: float) -> float:\n",
    "    return max(0.0, min(1.0, 1.0 - float(noise_ratio)))\n",
    "\n",
    "\n",
    "def fmt(x: float, d: int) -> str:\n",
    "    return f\"{float(x):.{d}f}\"\n",
    "\n",
    "\n",
    "def make_table_preview_png(df: pd.DataFrame, out_path: Path, dpi: int, max_rows: int = 16) -> None:\n",
    "    view = df.head(max_rows).copy()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis(\"off\")\n",
    "    tbl = ax.table(cellText=view.values, colLabels=view.columns, loc=\"center\")\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(8)\n",
    "    tbl.scale(1, 1.2)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def load_inputs(settings: Settings):\n",
    "    root = settings.folder_store.resolve()\n",
    "    d1 = root / \"data_to_report\" / \"data1\"\n",
    "    d2 = root / \"data_to_report\" / \"data2\"\n",
    "\n",
    "    p_all = d1 / \"experiment_summary_all.csv\"\n",
    "    p_alpha = d1 / \"alpha_sweep_ready.csv\"\n",
    "    p_best_alpha = d1 / \"best_alpha_per_noise.csv\"\n",
    "    p_main = d1 / \"main_table_ready.csv\"\n",
    "\n",
    "    p_fq = d2 / \"filter_quality_all.csv\"\n",
    "    p_cb_long = d2 / \"class_balance_long.csv\"\n",
    "    p_cb_cn = d2 / \"class_balance_clean_noisy_long.csv\"\n",
    "\n",
    "    ensure_exists(p_all, \"Data1 experiment_summary_all.csv\")\n",
    "    ensure_exists(p_alpha, \"Data1 alpha_sweep_ready.csv\")\n",
    "    ensure_exists(p_best_alpha, \"Data1 best_alpha_per_noise.csv\")\n",
    "    ensure_exists(p_main, \"Data1 main_table_ready.csv\")\n",
    "    ensure_exists(p_fq, \"Data2 filter_quality_all.csv\")\n",
    "\n",
    "    df_all = read_csv_normalized(\n",
    "        p_all,\n",
    "        force_str_cols={\"timestamp\", \"source_path\", \"run_id\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "    df_alpha = read_csv_normalized(\n",
    "        p_alpha,\n",
    "        force_str_cols={\"source_path\", \"run_id\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "    df_best_alpha = read_csv_normalized(\n",
    "        p_best_alpha,\n",
    "        force_str_cols={\"source_path\", \"run_id\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "    df_main = read_csv_normalized(\n",
    "        p_main,\n",
    "        force_str_cols={\"run_id\", \"Method\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "    df_fq = read_csv_normalized(\n",
    "        p_fq,\n",
    "        force_str_cols={\"source_path\", \"run_id\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "\n",
    "    df_cb_long = pd.DataFrame()\n",
    "    if p_cb_long.exists():\n",
    "        df_cb_long = read_csv_normalized(\n",
    "            p_cb_long,\n",
    "            force_str_cols={\"run_id\", \"class_name\"},\n",
    "            numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "        )\n",
    "\n",
    "    df_cb_cn = pd.DataFrame()\n",
    "    if p_cb_cn.exists():\n",
    "        df_cb_cn = read_csv_normalized(\n",
    "            p_cb_cn,\n",
    "            force_str_cols={\"run_id\", \"class_name\"},\n",
    "            numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "        )\n",
    "\n",
    "    return df_all, df_alpha, df_best_alpha, df_main, df_fq, df_cb_long, df_cb_cn\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Table 3: Filter summary\n",
    "# -------------------------\n",
    "def build_filter_summary_table(\n",
    "    settings: Settings,\n",
    "    df_best_alpha: pd.DataFrame,\n",
    "    df_fq: pd.DataFrame,\n",
    "    df_all: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a compact filter summary at the selected best alpha + selected best iteration:\n",
    "      columns:\n",
    "        run_id, noise_ratio, best_alpha, best_iteration,\n",
    "        kept_ratio, training_samples_used,\n",
    "        precision_kept, recall_clean_total, f1_clean_total\n",
    "    \"\"\"\n",
    "    required_best = {\"run_id\", \"noise_ratio\", \"best_alpha\"}\n",
    "    if not required_best.issubset(df_best_alpha.columns):\n",
    "        raise ValueError(f\"best_alpha_per_noise missing {sorted(required_best - set(df_best_alpha.columns))}\")\n",
    "\n",
    "    required_fq = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", \"precision_kept\", \"recall_clean_total\", \"f1_clean_total\"}\n",
    "    if not required_fq.issubset(df_fq.columns):\n",
    "        raise ValueError(f\"filter_quality_all missing {sorted(required_fq - set(df_fq.columns))}\")\n",
    "\n",
    "    required_all = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", settings.kept_ratio_col, \"training_samples_used\"}\n",
    "    if not required_all.issubset(df_all.columns):\n",
    "        raise ValueError(f\"experiment_summary_all missing {sorted(required_all - set(df_all.columns))}\")\n",
    "\n",
    "    rows: List[dict] = []\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "\n",
    "        best_iter = None\n",
    "        if \"best_iteration\" in df_best_alpha.columns and pd.notna(r.get(\"best_iteration\")):\n",
    "            best_iter = int(r[\"best_iteration\"])\n",
    "\n",
    "        fq_sub = df_fq[(df_fq[\"run_id\"] == run_id) & (df_fq[\"noise_ratio\"] == noise) & (df_fq[\"alpha\"] == best_alpha)].copy()\n",
    "        if fq_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Prefer selected best_iter; fallback to max F1 iteration\n",
    "        if best_iter is not None:\n",
    "            fq_row = fq_sub[fq_sub[\"iteration\"] == best_iter].head(1)\n",
    "            if fq_row.empty:\n",
    "                fq_row = fq_sub.loc[[fq_sub[\"f1_clean_total\"].idxmax()]]\n",
    "                best_iter = int(fq_row[\"iteration\"].iloc[0])\n",
    "            else:\n",
    "                fq_row = fq_row\n",
    "        else:\n",
    "            fq_row = fq_sub.loc[[fq_sub[\"f1_clean_total\"].idxmax()]]\n",
    "            best_iter = int(fq_row[\"iteration\"].iloc[0])\n",
    "\n",
    "        fq_row = fq_row.iloc[0]\n",
    "\n",
    "        # Get kept_ratio + training_samples_used from experiment summary at same (alpha, iter)\n",
    "        all_row = df_all[\n",
    "            (df_all[\"run_id\"] == run_id)\n",
    "            & (df_all[\"noise_ratio\"] == noise)\n",
    "            & (df_all[\"alpha\"] == best_alpha)\n",
    "            & (df_all[\"iteration\"] == best_iter)\n",
    "        ].head(1)\n",
    "\n",
    "        kept_ratio = float(all_row[settings.kept_ratio_col].iloc[0]) if not all_row.empty else float(\"nan\")\n",
    "        training_used = int(all_row[\"training_samples_used\"].iloc[0]) if not all_row.empty else int(-1)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"run_id\": run_id,\n",
    "                \"noise_ratio\": noise,\n",
    "                \"best_alpha\": best_alpha,\n",
    "                \"best_iteration\": best_iter,\n",
    "                \"kept_ratio\": kept_ratio,\n",
    "                \"training_samples_used\": training_used,\n",
    "                \"precision_kept\": float(fq_row[\"precision_kept\"]),\n",
    "                \"recall_clean_total\": float(fq_row[\"recall_clean_total\"]),\n",
    "                \"f1_clean_total\": float(fq_row[\"f1_clean_total\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"run_id\", \"noise_ratio\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Export tables + captions\n",
    "# -------------------------\n",
    "def export_tables_and_captions(\n",
    "    settings: Settings,\n",
    "    df_main: pd.DataFrame,\n",
    "    df_best_alpha: pd.DataFrame,\n",
    "    df_filter_summary: pd.DataFrame,\n",
    "    out_root: Path,\n",
    ") -> None:\n",
    "    tables_dir = out_root / \"tables\"\n",
    "    figs_dir = out_root / \"figures\"\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "    figs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_main.to_csv(tables_dir / \"main_table_ready.csv\", index=False)\n",
    "    df_best_alpha.to_csv(tables_dir / \"best_alpha_per_noise.csv\", index=False)\n",
    "    df_filter_summary.to_csv(tables_dir / \"filter_summary_table.csv\", index=False)\n",
    "\n",
    "    make_table_preview_png(df_main, figs_dir / \"main_table_preview.png\", dpi=settings.dpi, max_rows=16)\n",
    "\n",
    "    # captions.md\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Captions for Report\\n\\n\")\n",
    "    lines.append(\"## Tables\\n\\n\")\n",
    "    lines.append(\"- **Table 1. Main results (Baseline vs Ours)** — `tables/main_table_ready.csv`\\n\")\n",
    "    lines.append(\"- **Table 2. Selected best EMA momentum α per noise (by noisy-val)** — `tables/best_alpha_per_noise.csv`\\n\")\n",
    "    lines.append(\"- **Table 3. Filter quality summary at selected best α & iteration (Precision/Recall/F1)** — `tables/filter_summary_table.csv`\\n\\n\")\n",
    "\n",
    "    lines.append(\"## Figures\\n\\n\")\n",
    "    lines.append(\"- Notation: ★ = selected by noisy-val (your selection rule), ▲ = oracle best by test_acc (analysis only)\\n\\n\")\n",
    "\n",
    "    pairs = df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"])[[\"run_id\", \"noise_ratio\"]]\n",
    "    for _, rr in pairs.sort_values([\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(rr[\"run_id\"])\n",
    "        noise = float(rr[\"noise_ratio\"])\n",
    "        lines.append(f\"### Run `{run_id}` — Noise `{noise}`\\n\\n\")\n",
    "        rels = [\n",
    "            (\"Figure A. Ablation α vs test accuracy\", f\"ablation_alpha_testacc/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure B. Kept ratio vs iteration (best α) + expected clean ratio (1-noise)\", f\"kept_ratio_vs_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure C. Test accuracy vs iteration (best α)\", f\"testacc_vs_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure D. Val noisy vs Val orig (scatter across α)\", f\"val_noisy_vs_val_orig_scatter/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure E. Val curves across iterations (best α): val_noisy vs val_orig\", f\"val_noisy_vs_val_orig_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure F. Filter quality across iterations (best α): Precision/Recall/F1\", f\"filter_quality_prf_vs_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure G. Class balance at best iteration (kept samples)\", f\"class_balance_bar_best/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Figure H. Class kept clean vs noisy at best iteration\", f\"class_balance_clean_noisy_best/{run_id}/noise_{noise}.png\"),\n",
    "        ]\n",
    "        for title, rel in rels:\n",
    "            if (figs_dir / rel).exists():\n",
    "                lines.append(f\"- **{title}** — `figures/{rel}`\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "\n",
    "    (out_root / \"captions.md\").write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plot helpers (minimal annotation)\n",
    "# -------------------------\n",
    "def oracle_best_alpha_by_testacc(df_alpha: pd.DataFrame, run_id: str, noise: float, test_acc_col: str) -> Optional[pd.Series]:\n",
    "    sub = df_alpha[(df_alpha[\"run_id\"] == run_id) & (df_alpha[\"noise_ratio\"] == noise)].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    sub = sub.dropna(subset=[test_acc_col])\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    return sub.loc[sub[test_acc_col].idxmax()]\n",
    "\n",
    "\n",
    "def oracle_best_iter_by_testacc(df_all: pd.DataFrame, run_id: str, noise: float, alpha: float, test_acc_col: str) -> Optional[pd.Series]:\n",
    "    sub = df_all[\n",
    "        (df_all[\"run_id\"] == run_id)\n",
    "        & (df_all[\"noise_ratio\"] == noise)\n",
    "        & (df_all[\"alpha\"] == alpha)\n",
    "    ].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    sub = sub.dropna(subset=[test_acc_col])\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    return sub.loc[sub[test_acc_col].idxmax()]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plots (minimal)\n",
    "# -------------------------\n",
    "def plot_ablation_alpha_testacc(settings: Settings, df_alpha: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", settings.test_acc_col}\n",
    "    missing = req - set(df_alpha.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"alpha_sweep_ready.csv missing columns: {sorted(missing)}\")\n",
    "\n",
    "    for (run_id_obj, noise_obj), sub in df_alpha.groupby([\"run_id\", \"noise_ratio\"]):\n",
    "        run_id = str(run_id_obj)\n",
    "        noise = float(noise_obj)\n",
    "        sub = sub.sort_values(\"alpha\").copy()\n",
    "\n",
    "        # Selected alpha (★) from best_alpha_per_noise\n",
    "        sel_row = df_best_alpha[(df_best_alpha[\"run_id\"] == run_id) & (df_best_alpha[\"noise_ratio\"] == noise)].head(1)\n",
    "        best_alpha = float(sel_row[\"best_alpha\"].iloc[0]) if not sel_row.empty else None\n",
    "        best_iter = int(sel_row[\"best_iteration\"].iloc[0]) if (not sel_row.empty and \"best_iteration\" in sel_row.columns and pd.notna(sel_row[\"best_iteration\"].iloc[0])) else None\n",
    "\n",
    "        # Oracle alpha by test (▲)\n",
    "        oracle = oracle_best_alpha_by_testacc(df_alpha, run_id, noise, settings.test_acc_col)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"alpha\"], sub[settings.test_acc_col], marker=\"o\")\n",
    "        plt.xlabel(\"EMA momentum α\")\n",
    "        # plt.ylabel(settings.test_acc_col)\n",
    "        plt.ylabel(\"test acc\")\n",
    "        plt.title(f\"Ablation: α vs test acc | run={run_id} | noise={noise}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Mark selected\n",
    "        if best_alpha is not None:\n",
    "            sel_point = sub[sub[\"alpha\"] == best_alpha].head(1)\n",
    "            if not sel_point.empty:\n",
    "                y_sel = float(sel_point[settings.test_acc_col].iloc[0])\n",
    "                plt.scatter([best_alpha], [y_sel], marker=\"*\", s=180, label=\"Selected (★) by noisy-val\")\n",
    "                plt.axvline(best_alpha, linestyle=\"--\")\n",
    "                it_txt = f\", iter={best_iter}\" if best_iter is not None else \"\"\n",
    "                plt.annotate(\n",
    "                    f\"★ α={fmt(best_alpha, 2)}\\nacc={fmt(y_sel, settings.ann_decimals)}{it_txt}\",\n",
    "                    (best_alpha, y_sel),\n",
    "                    fontsize=9,\n",
    "                    xytext=(10, -25),\n",
    "                    textcoords=\"offset points\",\n",
    "                )\n",
    "\n",
    "        # Mark oracle\n",
    "        if oracle is not None:\n",
    "            a_or = float(oracle[\"alpha\"])\n",
    "            y_or = float(oracle[settings.test_acc_col])\n",
    "            plt.scatter([a_or], [y_or], marker=\"^\", s=90, label=\"Oracle (▲) by test_acc\")\n",
    "            plt.annotate(\n",
    "                f\"▲ α={fmt(a_or, 2)}\\nacc={fmt(y_or, settings.ann_decimals)}\",\n",
    "                (a_or, y_or),\n",
    "                fontsize=9,\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "\n",
    "        plt.legend(loc=\"best\", fontsize=8)\n",
    "        savefig(figs_dir / \"ablation_alpha_testacc\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_kept_ratio_vs_iteration(settings: Settings, df_all: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", settings.kept_ratio_col}\n",
    "    missing = req - set(df_all.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"experiment_summary_all.csv missing columns: {sorted(missing)}\")\n",
    "\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "        best_iter = int(r[\"best_iteration\"]) if (\"best_iteration\" in df_best_alpha.columns and pd.notna(r.get(\"best_iteration\"))) else None\n",
    "\n",
    "        sub = df_all[\n",
    "            (df_all[\"run_id\"] == run_id)\n",
    "            & (df_all[\"noise_ratio\"] == noise)\n",
    "            & (df_all[\"alpha\"] == best_alpha)\n",
    "        ].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        sub = sub.sort_values(\"iteration\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"iteration\"], sub[settings.kept_ratio_col], marker=\"o\", label=\"kept_ratio\")\n",
    "        plt.axhline(expected_clean_ratio(noise), linestyle=\"--\", label=\"expected clean ratio (1-noise)\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(settings.kept_ratio_col)\n",
    "        plt.title(f\"Kept ratio vs iteration | run={run_id} | noise={noise} | selected α={fmt(best_alpha, 2)}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if best_iter is not None:\n",
    "            row_it = sub[sub[\"iteration\"] == best_iter].head(1)\n",
    "            if not row_it.empty:\n",
    "                y = float(row_it[settings.kept_ratio_col].iloc[0])\n",
    "                plt.scatter([best_iter], [y], marker=\"*\", s=180, label=\"Selected iter (★)\")\n",
    "                plt.annotate(\n",
    "                    f\"★ iter={best_iter}\\nkept={fmt(y, settings.ann_decimals)}\",\n",
    "                    (best_iter, y),\n",
    "                    fontsize=9,\n",
    "                    xytext=(10, 10),\n",
    "                    textcoords=\"offset points\",\n",
    "                )\n",
    "\n",
    "        plt.legend(loc=\"best\", fontsize=8)\n",
    "        savefig(figs_dir / \"kept_ratio_vs_iteration\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_testacc_vs_iteration(settings: Settings, df_all: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", settings.test_acc_col}\n",
    "    missing = req - set(df_all.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"experiment_summary_all.csv missing columns: {sorted(missing)}\")\n",
    "\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "        best_iter = int(r[\"best_iteration\"]) if (\"best_iteration\" in df_best_alpha.columns and pd.notna(r.get(\"best_iteration\"))) else None\n",
    "\n",
    "        sub = df_all[\n",
    "            (df_all[\"run_id\"] == run_id)\n",
    "            & (df_all[\"noise_ratio\"] == noise)\n",
    "            & (df_all[\"alpha\"] == best_alpha)\n",
    "        ].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        sub = sub.sort_values(\"iteration\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"iteration\"], sub[settings.test_acc_col], marker=\"o\", label=\"test_acc\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        # plt.ylabel(settings.test_acc_col)\n",
    "        plt.ylabel(\"test acc\")\n",
    "        \n",
    "        plt.title(f\"Test acc vs iteration | run={run_id} | noise={noise} | selected α={fmt(best_alpha, 2)}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if best_iter is not None:\n",
    "            row_it = sub[sub[\"iteration\"] == best_iter].head(1)\n",
    "            if not row_it.empty:\n",
    "                y_sel = float(row_it[settings.test_acc_col].iloc[0])\n",
    "                plt.scatter([best_iter], [y_sel], marker=\"*\", s=180, label=\"Selected iter (★)\")\n",
    "                plt.annotate(\n",
    "                    f\"★ iter={best_iter}\\nacc={fmt(y_sel, settings.ann_decimals)}\",\n",
    "                    (best_iter, y_sel),\n",
    "                    fontsize=9,\n",
    "                    xytext=(10, -25),\n",
    "                    textcoords=\"offset points\",\n",
    "                )\n",
    "\n",
    "        oracle_it = oracle_best_iter_by_testacc(df_all, run_id, noise, best_alpha, settings.test_acc_col)\n",
    "        if oracle_it is not None:\n",
    "            it_or = int(oracle_it[\"iteration\"])\n",
    "            y_or = float(oracle_it[settings.test_acc_col])\n",
    "            plt.scatter([it_or], [y_or], marker=\"^\", s=90, label=\"Oracle iter (▲) by test_acc\")\n",
    "            plt.annotate(\n",
    "                f\"▲ iter={it_or}\\nacc={fmt(y_or, settings.ann_decimals)}\",\n",
    "                (it_or, y_or),\n",
    "                fontsize=9,\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "\n",
    "        plt.legend(loc=\"best\", fontsize=8)\n",
    "        savefig(figs_dir / \"testacc_vs_iteration\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_val_noisy_vs_val_orig_scatter(settings: Settings, df_alpha: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", settings.val_noisy_col, settings.val_orig_col, settings.test_acc_col}\n",
    "    missing = req - set(df_alpha.columns)\n",
    "    if missing:\n",
    "        print(f\"[WARN] Skip val scatter: missing {sorted(missing)}\")\n",
    "        return\n",
    "\n",
    "    for (run_id_obj, noise_obj), sub in df_alpha.groupby([\"run_id\", \"noise_ratio\"]):\n",
    "        run_id = str(run_id_obj)\n",
    "        noise = float(noise_obj)\n",
    "        sub = sub.copy()\n",
    "\n",
    "        sel_row = df_best_alpha[(df_best_alpha[\"run_id\"] == run_id) & (df_best_alpha[\"noise_ratio\"] == noise)].head(1)\n",
    "        best_alpha = float(sel_row[\"best_alpha\"].iloc[0]) if not sel_row.empty else None\n",
    "\n",
    "        oracle = oracle_best_alpha_by_testacc(df_alpha, run_id, noise, settings.test_acc_col)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(sub[settings.val_noisy_col], sub[settings.val_orig_col])\n",
    "        plt.xlabel(f\"{settings.val_noisy_col} (best-by-noisy-val)\")\n",
    "        plt.ylabel(settings.val_orig_col)\n",
    "        plt.title(f\"Val relation across α | run={run_id} | noise={noise}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if best_alpha is not None:\n",
    "            p_sel = sub[sub[\"alpha\"] == best_alpha].head(1)\n",
    "            if not p_sel.empty:\n",
    "                x_sel = float(p_sel[settings.val_noisy_col].iloc[0])\n",
    "                y_sel = float(p_sel[settings.val_orig_col].iloc[0])\n",
    "                plt.scatter([x_sel], [y_sel], marker=\"*\", s=180, label=\"Selected α (★)\")\n",
    "                plt.annotate(\n",
    "                    f\"★ α={fmt(best_alpha, 2)}\",\n",
    "                    (x_sel, y_sel),\n",
    "                    fontsize=9,\n",
    "                    xytext=(10, -20),\n",
    "                    textcoords=\"offset points\",\n",
    "                )\n",
    "\n",
    "        if oracle is not None:\n",
    "            a_or = float(oracle[\"alpha\"])\n",
    "            x_or = float(oracle[settings.val_noisy_col])\n",
    "            y_or = float(oracle[settings.val_orig_col])\n",
    "            plt.scatter([x_or], [y_or], marker=\"^\", s=90, label=\"Oracle α (▲)\")\n",
    "            plt.annotate(\n",
    "                f\"▲ α={fmt(a_or, 2)}\",\n",
    "                (x_or, y_or),\n",
    "                fontsize=9,\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "            )\n",
    "\n",
    "        plt.legend(loc=\"best\", fontsize=8)\n",
    "        savefig(figs_dir / \"val_noisy_vs_val_orig_scatter\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_val_noisy_vs_val_orig_iteration(settings: Settings, df_all: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", settings.val_noisy_col, settings.val_orig_col}\n",
    "    missing = req - set(df_all.columns)\n",
    "    if missing:\n",
    "        print(f\"[WARN] Skip val curves: missing {sorted(missing)}\")\n",
    "        return\n",
    "\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "        best_iter = int(r[\"best_iteration\"]) if (\"best_iteration\" in df_best_alpha.columns and pd.notna(r.get(\"best_iteration\"))) else None\n",
    "\n",
    "        sub = df_all[\n",
    "            (df_all[\"run_id\"] == run_id)\n",
    "            & (df_all[\"noise_ratio\"] == noise)\n",
    "            & (df_all[\"alpha\"] == best_alpha)\n",
    "        ].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        sub = sub.sort_values(\"iteration\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"iteration\"], sub[settings.val_noisy_col], marker=\"o\", label=settings.val_noisy_col)\n",
    "        plt.plot(sub[\"iteration\"], sub[settings.val_orig_col], marker=\"o\", label=settings.val_orig_col)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Validation accuracy\")\n",
    "        plt.title(f\"Val curves vs iteration | run={run_id} | noise={noise} | selected α={fmt(best_alpha, 2)}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if best_iter is not None:\n",
    "            plt.axvline(best_iter, linestyle=\"--\")\n",
    "            plt.annotate(f\"★ iter={best_iter}\", (best_iter, float(sub[[settings.val_noisy_col, settings.val_orig_col]].max().max())),\n",
    "                         fontsize=9, xytext=(8, 0), textcoords=\"offset points\")\n",
    "\n",
    "        savefig(figs_dir / \"val_noisy_vs_val_orig_iteration\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_filter_quality_prf(settings: Settings, df_fq: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", \"precision_kept\", \"recall_clean_total\", \"f1_clean_total\"}\n",
    "    missing = req - set(df_fq.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"filter_quality_all.csv missing columns: {sorted(missing)}\")\n",
    "\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "        best_iter = int(r[\"best_iteration\"]) if (\"best_iteration\" in df_best_alpha.columns and pd.notna(r.get(\"best_iteration\"))) else None\n",
    "\n",
    "        sub = df_fq[\n",
    "            (df_fq[\"run_id\"] == run_id)\n",
    "            & (df_fq[\"noise_ratio\"] == noise)\n",
    "            & (df_fq[\"alpha\"] == best_alpha)\n",
    "        ].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        sub = sub.sort_values(\"iteration\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"iteration\"], sub[\"precision_kept\"], marker=\"o\", label=\"precision_kept\")\n",
    "        plt.plot(sub[\"iteration\"], sub[\"recall_clean_total\"], marker=\"o\", label=\"recall_clean_total\")\n",
    "        plt.plot(sub[\"iteration\"], sub[\"f1_clean_total\"], marker=\"o\", label=\"f1_clean_total\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(f\"Filter quality P/R/F1 | run={run_id} | noise={noise} | selected α={fmt(best_alpha, 2)}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        if best_iter is not None:\n",
    "            plt.axvline(best_iter, linestyle=\"--\")\n",
    "            row_it = sub[sub[\"iteration\"] == best_iter].head(1)\n",
    "            if not row_it.empty:\n",
    "                y = float(row_it[\"f1_clean_total\"].iloc[0])\n",
    "                plt.scatter([best_iter], [y], marker=\"*\", s=180, label=\"Selected iter (★) on F1\")\n",
    "                plt.annotate(f\"★ F1={fmt(y, settings.ann_decimals)}\", (best_iter, y),\n",
    "                             fontsize=9, xytext=(10, 10), textcoords=\"offset points\")\n",
    "\n",
    "        savefig(figs_dir / \"filter_quality_prf_vs_iteration\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_class_balance_best(settings: Settings, df_cb_long: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    if df_cb_long.empty:\n",
    "        return\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", \"class_name\", \"kept_count\"}\n",
    "    if not req.issubset(df_cb_long.columns):\n",
    "        return\n",
    "    if \"best_iteration\" not in df_best_alpha.columns:\n",
    "        return\n",
    "\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "        best_iter = int(r[\"best_iteration\"]) if pd.notna(r.get(\"best_iteration\")) else None\n",
    "        if best_iter is None:\n",
    "            continue\n",
    "\n",
    "        sub = df_cb_long[\n",
    "            (df_cb_long[\"run_id\"] == run_id)\n",
    "            & (df_cb_long[\"noise_ratio\"] == noise)\n",
    "            & (df_cb_long[\"alpha\"] == best_alpha)\n",
    "            & (df_cb_long[\"iteration\"] == best_iter)\n",
    "        ].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub = sub.sort_values(\"class_name\")\n",
    "        plt.figure()\n",
    "        plt.bar(sub[\"class_name\"].astype(str), sub[\"kept_count\"])\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Kept count\")\n",
    "        plt.title(f\"Class balance (kept) | run={run_id} | noise={noise}\\nselected α={fmt(best_alpha, 2)}, selected iter={best_iter}\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "        savefig(figs_dir / \"class_balance_bar_best\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "def plot_class_balance_clean_noisy_best(settings: Settings, df_cb_cn: pd.DataFrame, df_best_alpha: pd.DataFrame, figs_dir: Path) -> None:\n",
    "    if df_cb_cn.empty:\n",
    "        return\n",
    "    req = {\"run_id\", \"noise_ratio\", \"alpha\", \"iteration\", \"class_name\", \"kept_count_clean\", \"kept_count_noisy\"}\n",
    "    if not req.issubset(df_cb_cn.columns):\n",
    "        return\n",
    "    if \"best_iteration\" not in df_best_alpha.columns:\n",
    "        return\n",
    "\n",
    "    for _, r in df_best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        best_alpha = float(r[\"best_alpha\"])\n",
    "        best_iter = int(r[\"best_iteration\"]) if pd.notna(r.get(\"best_iteration\")) else None\n",
    "        if best_iter is None:\n",
    "            continue\n",
    "\n",
    "        sub = df_cb_cn[\n",
    "            (df_cb_cn[\"run_id\"] == run_id)\n",
    "            & (df_cb_cn[\"noise_ratio\"] == noise)\n",
    "            & (df_cb_cn[\"alpha\"] == best_alpha)\n",
    "            & (df_cb_cn[\"iteration\"] == best_iter)\n",
    "        ].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub = sub.sort_values(\"class_name\")\n",
    "        plt.figure()\n",
    "        plt.bar(sub[\"class_name\"].astype(str), sub[\"kept_count_clean\"], label=\"clean_kept\")\n",
    "        plt.bar(sub[\"class_name\"].astype(str), sub[\"kept_count_noisy\"], bottom=sub[\"kept_count_clean\"], label=\"noisy_kept\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Kept count\")\n",
    "        plt.title(f\"Class kept clean vs noisy | run={run_id} | noise={noise}\\nselected α={fmt(best_alpha, 2)}, selected iter={best_iter}\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "        savefig(figs_dir / \"class_balance_clean_noisy_best\" / run_id / f\"noise_{noise}.png\", settings.dpi)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main pipeline\n",
    "# -------------------------\n",
    "def generate_report_assets(settings: Settings) -> Path:\n",
    "    root = settings.folder_store.resolve()\n",
    "    out_root = root / settings.out_dir\n",
    "    figs_dir = out_root / \"figures\"\n",
    "    figs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_all, df_alpha, df_best_alpha, df_main, df_fq, df_cb_long, df_cb_cn = load_inputs(settings)\n",
    "\n",
    "    # Table 3\n",
    "    df_filter_summary = build_filter_summary_table(settings, df_best_alpha, df_fq, df_all)\n",
    "\n",
    "    # Export tables + captions\n",
    "    export_tables_and_captions(settings, df_main, df_best_alpha, df_filter_summary, out_root)\n",
    "\n",
    "    # Figures\n",
    "    plot_ablation_alpha_testacc(settings, df_alpha, df_best_alpha, figs_dir)\n",
    "    plot_kept_ratio_vs_iteration(settings, df_all, df_best_alpha, figs_dir)\n",
    "    plot_testacc_vs_iteration(settings, df_all, df_best_alpha, figs_dir)\n",
    "    plot_val_noisy_vs_val_orig_scatter(settings, df_alpha, df_best_alpha, figs_dir)\n",
    "    plot_val_noisy_vs_val_orig_iteration(settings, df_all, df_best_alpha, figs_dir)\n",
    "    plot_filter_quality_prf(settings, df_fq, df_best_alpha, figs_dir)\n",
    "    plot_class_balance_best(settings, df_cb_long, df_best_alpha, figs_dir)\n",
    "    plot_class_balance_clean_noisy_best(settings, df_cb_cn, df_best_alpha, figs_dir)\n",
    "\n",
    "    (out_root / \"MANIFEST.txt\").write_text(\n",
    "        \"Generated tables/figures for report.\\n\"\n",
    "        \"- tables/main_table_ready.csv\\n\"\n",
    "        \"- tables/best_alpha_per_noise.csv\\n\"\n",
    "        \"- tables/filter_summary_table.csv\\n\"\n",
    "        \"- figures/*\\n\"\n",
    "        \"- captions.md\\n\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    return out_root\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====== INPUT SETTINGS (edit here) ======\n",
    "    folder_store = \"store_output_cifar10_iter_ema_noise_validation_v2\"\n",
    "    # =======================================\n",
    "\n",
    "    out = generate_report_assets(Settings(folder_store=Path(folder_store)))\n",
    "    print(f\"[OK] Report assets written to: {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55c4c7",
   "metadata": {},
   "source": [
    "# 4 finalize_report_package.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ffd9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Final report package written to: /mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/notebooks/store_output_cifar10_iter_ema_noise_validation_v2/report_assets\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Finalize report package (tables + results template) from report_assets.\n",
    "\n",
    "Prerequisite:\n",
    "  - Run generate_report_assets.py first to create:\n",
    "      folder_store/report_assets/tables/*.csv\n",
    "      folder_store/report_assets/figures/**.png\n",
    "\n",
    "This script generates:\n",
    "  folder_store/report_assets/tables/\n",
    "    main_table_ready.xlsx\n",
    "    best_alpha_per_noise.xlsx\n",
    "    main_table_ready.tex\n",
    "    best_alpha_per_noise.tex\n",
    "\n",
    "  folder_store/report_assets/\n",
    "    results.md   (template to paste into Word/LaTeX, with links to figures/tables)\n",
    "\n",
    "Notes:\n",
    "- No seaborn used. No custom colors. PEP8-friendly.\n",
    "- Excel formatting uses openpyxl.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Robust CSV normalization\n",
    "# -------------------------\n",
    "def read_csv_normalized(\n",
    "    path: Path,\n",
    "    force_str_cols: Optional[Set[str]] = None,\n",
    "    numeric_ratio_threshold: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV and normalize:\n",
    "      - replace comma decimals -> dot decimals for object cols\n",
    "      - convert columns to numeric when they look numeric-like\n",
    "      - keep non-numeric columns as strings\n",
    "    \"\"\"\n",
    "    if force_str_cols is None:\n",
    "        force_str_cols = set()\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in force_str_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "            continue\n",
    "\n",
    "        s = df[col]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_object_dtype(s):\n",
    "            s_str = s.astype(str).str.replace(\",\", \".\", regex=False)\n",
    "        else:\n",
    "            s_str = s.astype(str)\n",
    "\n",
    "        s_num = pd.to_numeric(s_str, errors=\"coerce\")\n",
    "\n",
    "        s_norm = s_str.str.strip()\n",
    "        non_empty = s_norm.ne(\"\") & s_norm.str.lower().ne(\"nan\")\n",
    "        denom = int(non_empty.sum())\n",
    "        if denom == 0:\n",
    "            df[col] = s_str\n",
    "            continue\n",
    "\n",
    "        ok = int(s_num[non_empty].notna().sum())\n",
    "        if ok / denom >= numeric_ratio_threshold:\n",
    "            df[col] = s_num\n",
    "        else:\n",
    "            df[col] = s_str\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "@dataclass(frozen=True)\n",
    "class Settings:\n",
    "    folder_store: Path\n",
    "    out_dir: str = \"report_assets\"\n",
    "    numeric_ratio_threshold: float = 0.95\n",
    "    float_decimals: int = 4\n",
    "\n",
    "    # Table files expected from previous step\n",
    "    main_table_csv: str = \"main_table_ready.csv\"\n",
    "    best_alpha_csv: str = \"best_alpha_per_noise.csv\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Excel helpers (openpyxl)\n",
    "# -------------------------\n",
    "def _is_float_like(col: pd.Series) -> bool:\n",
    "    return pd.api.types.is_float_dtype(col) or pd.api.types.is_integer_dtype(col)\n",
    "\n",
    "\n",
    "def _best_width_for_values(values: List[str], min_w: int = 10, max_w: int = 40) -> int:\n",
    "    if not values:\n",
    "        return min_w\n",
    "    w = max(len(v) for v in values)\n",
    "    return max(min_w, min(max_w, w + 2))\n",
    "\n",
    "\n",
    "def write_excel_pretty(df: pd.DataFrame, out_path: Path, sheet_name: str, float_decimals: int) -> None:\n",
    "    \"\"\"\n",
    "    Write DataFrame to Excel with clean report-friendly formatting:\n",
    "      - header bold + centered\n",
    "      - freeze top row\n",
    "      - autofilter\n",
    "      - reasonable column widths\n",
    "      - numeric format for float columns\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = sheet_name[:31]  # Excel sheet name limit\n",
    "\n",
    "    # Write header\n",
    "    header_font = Font(bold=True)\n",
    "    center = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "\n",
    "    for j, col_name in enumerate(df.columns, start=1):\n",
    "        cell = ws.cell(row=1, column=j, value=str(col_name))\n",
    "        cell.font = header_font\n",
    "        cell.alignment = center\n",
    "\n",
    "    # Write rows\n",
    "    for i, row in enumerate(df.itertuples(index=False), start=2):\n",
    "        for j, value in enumerate(row, start=1):\n",
    "            ws.cell(row=i, column=j, value=value)\n",
    "\n",
    "    # Freeze header\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "    # Auto filter\n",
    "    ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "    # Column widths + formats\n",
    "    for j, col_name in enumerate(df.columns, start=1):\n",
    "        col_letter = get_column_letter(j)\n",
    "        col_series = df[col_name]\n",
    "\n",
    "        # width\n",
    "        sample_vals = [str(col_name)]\n",
    "        sample_vals += [str(v) for v in col_series.head(200).tolist()]\n",
    "        ws.column_dimensions[col_letter].width = _best_width_for_values(sample_vals)\n",
    "\n",
    "        # number format\n",
    "        if _is_float_like(col_series):\n",
    "            fmt = \"0\" if pd.api.types.is_integer_dtype(col_series) else (\"0.\" + \"0\" * float_decimals)\n",
    "            for i in range(2, 2 + len(df)):\n",
    "                ws.cell(row=i, column=j).number_format = fmt\n",
    "\n",
    "    wb.save(out_path)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LaTeX helpers\n",
    "# -------------------------\n",
    "def write_latex_table(\n",
    "    df: pd.DataFrame,\n",
    "    out_path: Path,\n",
    "    caption: str,\n",
    "    label: str,\n",
    "    float_decimals: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Export DataFrame to LaTeX (booktabs). You can paste into LaTeX directly.\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    float_fmt = f\"%.{float_decimals}f\"\n",
    "\n",
    "    latex_body = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        float_format=lambda x: float_fmt % x if pd.notna(x) else \"\",\n",
    "        longtable=False,\n",
    "        caption=caption,\n",
    "        label=label,\n",
    "        na_rep=\"\",\n",
    "        bold_rows=False,\n",
    "    )\n",
    "\n",
    "    # Ensure booktabs is used (pandas already uses \\toprule etc when available)\n",
    "    out_path.write_text(latex_body, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Results.md generator\n",
    "# -------------------------\n",
    "def _figure_path(figs_dir: Path, rel: str) -> Optional[Path]:\n",
    "    p = figs_dir / rel\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _collect_run_noise_pairs(best_alpha: pd.DataFrame) -> List[Tuple[str, float]]:\n",
    "    pairs = []\n",
    "    for _, r in best_alpha.drop_duplicates(subset=[\"run_id\", \"noise_ratio\"]).iterrows():\n",
    "        run_id = str(r[\"run_id\"])\n",
    "        noise = float(r[\"noise_ratio\"])\n",
    "        pairs.append((run_id, noise))\n",
    "    pairs.sort(key=lambda x: (x[0], x[1]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _safe_float(x) -> Optional[float]:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_results_md(\n",
    "    out_root: Path,\n",
    "    main_table: pd.DataFrame,\n",
    "    best_alpha: pd.DataFrame,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a report-ready Results template with:\n",
    "      - main table link\n",
    "      - per-noise summaries (best alpha, best iteration)\n",
    "      - links to figures generated earlier\n",
    "    \"\"\"\n",
    "    tables_dir = out_root / \"tables\"\n",
    "    figs_dir = out_root / \"figures\"\n",
    "\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Results\\n\")\n",
    "    lines.append(\"\\n\")\n",
    "    lines.append(\"## Summary of main results\\n\")\n",
    "    lines.append(\"\\n\")\n",
    "    lines.append(\n",
    "        f\"- Main table (CSV): `{(tables_dir / 'main_table_ready.csv').as_posix()}`\\n\"\n",
    "    )\n",
    "    if (tables_dir / \"main_table_ready.xlsx\").exists():\n",
    "        lines.append(\n",
    "            f\"- Main table (Excel): `{(tables_dir / 'main_table_ready.xlsx').as_posix()}`\\n\"\n",
    "        )\n",
    "    if (tables_dir / \"main_table_ready.tex\").exists():\n",
    "        lines.append(\n",
    "            f\"- Main table (LaTeX): `{(tables_dir / 'main_table_ready.tex').as_posix()}`\\n\"\n",
    "        )\n",
    "    lines.append(\"\\n\")\n",
    "\n",
    "    # Optional: show key gains from main table in text (compact)\n",
    "    # We keep it simple & robust: compute delta test_acc_reported if both methods exist.\n",
    "    if {\"noise_ratio\", \"Method\", \"test_acc_reported\"}.issubset(set(main_table.columns)):\n",
    "        lines.append(\"**Key observations (from the main table):**\\n\\n\")\n",
    "        try:\n",
    "            for noise, sub in main_table.groupby(\"noise_ratio\"):\n",
    "                base = sub[sub[\"Method\"].astype(str).str.lower() == \"baseline\"]\n",
    "                ours = sub[sub[\"Method\"].astype(str).str.lower() == \"ours\"]\n",
    "                if base.empty or ours.empty:\n",
    "                    continue\n",
    "                base_acc = _safe_float(base[\"test_acc_reported\"].iloc[0])\n",
    "                ours_acc = _safe_float(ours[\"test_acc_reported\"].iloc[0])\n",
    "                if base_acc is None or ours_acc is None:\n",
    "                    continue\n",
    "                delta = ours_acc - base_acc\n",
    "                lines.append(f\"- Noise={noise}: Δ test_acc_reported = {delta:.4f}\\n\")\n",
    "        except Exception:\n",
    "            lines.append(\"- (Could not auto-compute deltas reliably; check main table.)\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "\n",
    "    lines.append(\"## Detailed analysis by noise ratio\\n\\n\")\n",
    "\n",
    "    # Ensure required cols exist\n",
    "    req_cols = {\"run_id\", \"noise_ratio\", \"best_alpha\"}\n",
    "    if not req_cols.issubset(set(best_alpha.columns)):\n",
    "        lines.append(\n",
    "            \"⚠️ Missing required columns in best_alpha_per_noise.csv. \"\n",
    "            \"Expected: run_id, noise_ratio, best_alpha.\\n\"\n",
    "        )\n",
    "        out_root.joinpath(\"results.md\").write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    pairs = _collect_run_noise_pairs(best_alpha)\n",
    "\n",
    "    for run_id, noise in pairs:\n",
    "        row = best_alpha[(best_alpha[\"run_id\"] == run_id) & (best_alpha[\"noise_ratio\"] == noise)].head(1)\n",
    "        if row.empty:\n",
    "            continue\n",
    "\n",
    "        best_a = float(row[\"best_alpha\"].iloc[0])\n",
    "        best_it = int(row[\"best_iteration\"].iloc[0]) if \"best_iteration\" in row.columns and pd.notna(row[\"best_iteration\"].iloc[0]) else None\n",
    "\n",
    "        lines.append(f\"### Run: `{run_id}` — Noise ratio: `{noise}`\\n\\n\")\n",
    "        lines.append(f\"- Best EMA momentum α: **{best_a}**\\n\")\n",
    "        if best_it is not None:\n",
    "            lines.append(f\"- Best iteration (selected by noisy-val): **{best_it}**\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "\n",
    "        # Figures to embed (if exist)\n",
    "        fig_candidates = [\n",
    "            (\"Ablation α vs test acc\",\n",
    "             f\"ablation_alpha_testacc/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Kept ratio vs iteration (best α) + expected clean ratio line\",\n",
    "             f\"kept_ratio_vs_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Test acc vs iteration (best α)\",\n",
    "             f\"testacc_vs_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Noisy-val vs Orig-val (scatter across α)\",\n",
    "             f\"val_noisy_vs_val_orig_scatter/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Val curves across iterations (best α): val_noisy vs val_orig\",\n",
    "             f\"val_noisy_vs_val_orig_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Filter quality across iterations (best α): Precision/Recall/F1\",\n",
    "             f\"filter_quality_prf_vs_iteration/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Class balance at best iteration (kept samples)\",\n",
    "             f\"class_balance_bar_best/{run_id}/noise_{noise}.png\"),\n",
    "            (\"Class kept: clean vs noisy at best iteration\",\n",
    "             f\"class_balance_clean_noisy_best/{run_id}/noise_{noise}.png\"),\n",
    "        ]\n",
    "\n",
    "        for title, rel in fig_candidates:\n",
    "            p = _figure_path(figs_dir, rel)\n",
    "            if p is None:\n",
    "                continue\n",
    "            lines.append(f\"**{title}**\\n\\n\")\n",
    "            # Markdown image embed (relative to report_assets)\n",
    "            rel_path = Path(\"figures\") / Path(rel)\n",
    "            lines.append(f\"![{title}]({rel_path.as_posix()})\\n\\n\")\n",
    "\n",
    "        # Placeholders for interpretation paragraphs (you will fill)\n",
    "        lines.append(\"**Interpretation notes (fill in):**\\n\\n\")\n",
    "        lines.append(\"- Expected behavior: …\\n\")\n",
    "        lines.append(\"- Observed behavior: …\\n\")\n",
    "        lines.append(\"- Why EMA-Predict helps at this noise level: …\\n\")\n",
    "        lines.append(\"- Failure modes / limitations: …\\n\\n\")\n",
    "\n",
    "    out_root.joinpath(\"results.md\").write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def finalize_report(settings: Settings) -> Path:\n",
    "    root = settings.folder_store.resolve()\n",
    "    out_root = root / settings.out_dir\n",
    "\n",
    "    tables_dir = out_root / \"tables\"\n",
    "    figs_dir = out_root / \"figures\"\n",
    "\n",
    "    # Ensure report_assets exists\n",
    "    if not out_root.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing {out_root}. Run generate_report_assets.py first.\"\n",
    "        )\n",
    "    if not tables_dir.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing {tables_dir}. Run generate_report_assets.py first.\"\n",
    "        )\n",
    "\n",
    "    main_csv = tables_dir / settings.main_table_csv\n",
    "    best_alpha_csv = tables_dir / settings.best_alpha_csv\n",
    "    if not main_csv.exists():\n",
    "        raise FileNotFoundError(f\"Missing main table CSV: {main_csv}\")\n",
    "    if not best_alpha_csv.exists():\n",
    "        raise FileNotFoundError(f\"Missing best alpha CSV: {best_alpha_csv}\")\n",
    "\n",
    "    df_main = read_csv_normalized(\n",
    "        main_csv,\n",
    "        force_str_cols={\"run_id\", \"Method\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "    df_best_alpha = read_csv_normalized(\n",
    "        best_alpha_csv,\n",
    "        force_str_cols={\"run_id\"},\n",
    "        numeric_ratio_threshold=settings.numeric_ratio_threshold,\n",
    "    )\n",
    "\n",
    "    # Export Excel\n",
    "    write_excel_pretty(\n",
    "        df_main,\n",
    "        out_path=tables_dir / \"main_table_ready.xlsx\",\n",
    "        sheet_name=\"MainTable\",\n",
    "        float_decimals=settings.float_decimals,\n",
    "    )\n",
    "    write_excel_pretty(\n",
    "        df_best_alpha,\n",
    "        out_path=tables_dir / \"best_alpha_per_noise.xlsx\",\n",
    "        sheet_name=\"BestAlpha\",\n",
    "        float_decimals=settings.float_decimals,\n",
    "    )\n",
    "\n",
    "    # Export LaTeX\n",
    "    write_latex_table(\n",
    "        df_main,\n",
    "        out_path=tables_dir / \"main_table_ready.tex\",\n",
    "        caption=\"Main results: Baseline vs EMA-Predict (Ours).\",\n",
    "        label=\"tab:main_results\",\n",
    "        float_decimals=settings.float_decimals,\n",
    "    )\n",
    "    write_latex_table(\n",
    "        df_best_alpha,\n",
    "        out_path=tables_dir / \"best_alpha_per_noise.tex\",\n",
    "        caption=\"Selected best EMA momentum $\\\\alpha$ for each noise ratio.\",\n",
    "        label=\"tab:best_alpha\",\n",
    "        float_decimals=settings.float_decimals,\n",
    "    )\n",
    "\n",
    "    # Create results.md\n",
    "    generate_results_md(out_root=out_root, main_table=df_main, best_alpha=df_best_alpha)\n",
    "\n",
    "    # Manifest update\n",
    "    manifest = out_root / \"MANIFEST_finalize.txt\"\n",
    "    manifest.write_text(\n",
    "        \"Finalize report outputs generated:\\n\"\n",
    "        \"- tables/main_table_ready.xlsx\\n\"\n",
    "        \"- tables/best_alpha_per_noise.xlsx\\n\"\n",
    "        \"- tables/main_table_ready.tex\\n\"\n",
    "        \"- tables/best_alpha_per_noise.tex\\n\"\n",
    "        \"- results.md\\n\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    return out_root\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====== INPUT SETTINGS (edit here) ======\n",
    "    folder_store = \"store_output_cifar10_iter_ema_noise_validation_v2\"\n",
    "    # =======================================\n",
    "\n",
    "    out = finalize_report(Settings(folder_store=Path(folder_store)))\n",
    "    print(f\"[OK] Final report package written to: {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa526c-4619-4253-a2e8-720d7f9dd82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5b8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (self_ensembling)",
   "language": "python",
   "name": "self_ensembling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
