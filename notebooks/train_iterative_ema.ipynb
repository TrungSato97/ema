{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160e7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thư mục parent: /mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project\n",
      "Sys.path cập nhật: ['/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project', '/home/trungsato/miniconda3/envs/self_ensembling/lib/python310.zip', '/home/trungsato/miniconda3/envs/self_ensembling/lib/python3.10', '/home/trungsato/miniconda3/envs/self_ensembling/lib/python3.10/lib-dynload', '', '/home/trungsato/.local/lib/python3.10/site-packages', '/home/trungsato/miniconda3/envs/self_ensembling/lib/python3.10/site-packages']\n",
      "Original training data: 45000 samples\n",
      "Initial train_loader: 45000 samples\n",
      "train_full_loader: 45000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                              | 0/176 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                               | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                               | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                               | 0/34 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                               | 0/34 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                               | 0/34 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/34 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data: 45000 samples\n",
      "Initial train_loader: 45000 samples\n",
      "train_full_loader: 45000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|                                                                                                                                                                                  | 0/176 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data: 45000 samples\n",
      "Initial train_loader: 45000 samples\n",
      "train_full_loader: 45000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|                                                                                                                                                                                  | 0/176 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp and device.startswith(\"cuda\") else None\n",
      "Train:   0%|                                                                                                                                                                                   | 0/36 [00:00<?, ?it/s]/mnt/c/Users/truon/learning/ptit/research/trung/M_10_01_2025/code_v2/project/src/train_loop.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                                                                                                                                                      "
     ]
    }
   ],
   "source": [
    "# iterative_driver.py (main training driver)\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path  # Sử dụng Path để lấy parent dễ dàng và an toàn hơn\n",
    "\n",
    "# Lấy đường dẫn thư mục parent của current working directory\n",
    "parent_dir = Path(os.getcwd()).parent\n",
    "\n",
    "# Thêm vào sys.path (dùng str() để chuyển thành string)\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Kiểm tra để debug\n",
    "print(\"Thư mục parent:\", parent_dir)\n",
    "print(\"Sys.path cập nhật:\", sys.path)\n",
    "\n",
    "\n",
    "# from src.train_loop import train_iteration  # adjust import path\n",
    "# from src.dataset_utils import make_data_loaders  # adjust import path\n",
    "# from src.ema_utils import update_ema, save_npz, set_global_seed  # adjust import path\n",
    "# import logging\n",
    "from src.config import TrainConfig\n",
    "from src.dataset_utils import prepare_cifar_data, make_data_loaders\n",
    "from src.train_loop import train_iteration\n",
    "from src.ema_utils import update_ema, save_preds_npz, load_preds_npz\n",
    "from src.filter_utils import filter_by_ema\n",
    "from src.io_utils import make_dirs, save_dataframe_csv, save_npz\n",
    "from src.eda import plot_class_distribution, plot_confusion_matrix, plot_filter_ratios_over_iterations, plot_confidence_histogram\n",
    "from src.seed_utils import set_global_seed\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Assume config has:\n",
    "# config.ls_ratio_noise (list of floats or str), config.ls_alpha (list of floats), config.base_exp_dir (str)\n",
    "# config.max_iterations, config.seed, config.patience_iter, config.data_dir, etc.\n",
    "# Tạo config cho noise ratio này\n",
    "config = TrainConfig()\n",
    "base_exp_dir = Path(config.exp_dir)\n",
    "\n",
    "# original fallback csvs (in case per-noise csvs missing)\n",
    "paths_fallback = {\n",
    "    \"train_csv\": str(Path(config.data_dir) / \"csvs\" / \"train.csv\"),\n",
    "    \"val_csv\": str(Path(config.data_dir) / \"csvs\" / \"val.csv\"),\n",
    "    \"test_csv\": str(Path(config.data_dir) / \"csvs\" / \"test.csv\"),\n",
    "}\n",
    "\n",
    "# Loop over noise ratios and alphas\n",
    "# NOISE_RATIOS = [0.2, 0.4, 0.6, 0.8]  # 20%, 40%, 60%, 80% label noise\n",
    "# NOISE_RATIOS = [0.2, 0.4]  # 20%, 40%, 60%, 80% label noise\n",
    "# NOISE_RATIOS = [0.4]  # 20%, 40%, 60%, 80% label noise\n",
    "NOISE_RATIOS = [0.8]  # 20%, 40%, 60%, 80% label noise\n",
    "\n",
    "config.ls_ratio_noise = NOISE_RATIOS\n",
    "\n",
    "# config.ls_alpha = [0.5, 0.6,0.8,0.85,0.9,0.95,0.99]\n",
    "# config.ls_alpha = [0.2, 0.3, 0.5, 0.6,0.8,0.85,0.9,0.95,0.99]\n",
    "# config.ls_alpha = [ 0.2, 0.3, 0.5, 0.6,0.8]\n",
    "config.ls_alpha = [0.9,0.95,0.99]\n",
    "\n",
    "# config.max_iterations = 1\n",
    "# config.max_epochs_per_iter = 1\n",
    "# config.batch_size = 256\n",
    "# config.lr = 0.01\n",
    "\n",
    "for noise_ratio in config.ls_ratio_noise:\n",
    "    config.noise_ratio = noise_ratio\n",
    "    \n",
    "    \n",
    "    # find csv dir for this noise ratio\n",
    "    csv_dir = Path(config.data_dir) / \"csvs\" / f\"noise_{noise_ratio}\"\n",
    "    train_csv_path = csv_dir / \"train.csv\"\n",
    "    val_csv_path = csv_dir / \"val.csv\"\n",
    "    test_csv_path = csv_dir / \"test.csv\"\n",
    "\n",
    "    if not train_csv_path.exists():\n",
    "        logger.warning(\"Train CSV for noise %s not found at %s. Falling back to default csvs.\", noise_ratio, train_csv_path)\n",
    "        paths = paths_fallback\n",
    "    else:\n",
    "        # use csvs from this folder\n",
    "        paths = {\n",
    "            \"train_csv\": str(train_csv_path),\n",
    "            \"val_csv\": str(val_csv_path),\n",
    "            \"test_csv\": str(test_csv_path)\n",
    "        }\n",
    "\n",
    "    # read original_train_df for indexing etc.\n",
    "    original_train_df = pd.read_csv(paths[\"train_csv\"])\n",
    "    original_total_samples = len(original_train_df)\n",
    "\n",
    "    for alpha in config.ls_alpha:\n",
    "        # Tạo config cho noise ratio này\n",
    "        # set alpha in config for update_ema calls\n",
    "        config.alpha = alpha\n",
    "\n",
    "        # set experiment dir for this (noise_ratio, alpha)\n",
    "        config.exp_dir = str(base_exp_dir / f\"noise_{noise_ratio}\" / f\"alpha_{alpha}\")\n",
    "        os.makedirs(config.exp_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Starting experiment noise=%s alpha=%s exp_dir=%s\", noise_ratio, alpha, config.exp_dir)\n",
    "\n",
    "        # Create data loaders for this noise ratio\n",
    "        set_global_seed(config.seed, deterministic=True)\n",
    "        dls = make_data_loaders(paths[\"train_csv\"], paths[\"val_csv\"], paths[\"test_csv\"], config)\n",
    "        train_loader = dls['train']\n",
    "        val_loader = dls['val']\n",
    "        test_loader = dls['test']\n",
    "        train_full_loader = dls['train_full']\n",
    "\n",
    "        print(f\"Original training data: {original_total_samples} samples\")\n",
    "        print(f\"Initial train_loader: {len(train_loader.dataset)} samples\")\n",
    "        print(f\"train_full_loader: {len(train_full_loader.dataset)} samples\")\n",
    "\n",
    "        # Prepare experiment-level summary container\n",
    "        experiment_summary_rows = []\n",
    "        experiment_summary_path = Path(config.exp_dir) / \"experiment_summary.csv\"\n",
    "\n",
    "        z_ema_prev = None\n",
    "        best_overall_val_acc = -1.0\n",
    "        iter_no_improve = 0\n",
    "\n",
    "        # Iterations loop\n",
    "        for i in range(config.max_iterations):\n",
    "            logger.info(\"Starting iteration %d for noise=%s alpha=%s\", i, noise_ratio, alpha)\n",
    "\n",
    "            # If i > 0, use filtered train csv from prev iteration\n",
    "            if i > 0:\n",
    "                filtered_train_csv = Path(config.exp_dir) / f\"iteration_{i-1}\" / f\"train_kept_{i-1}.csv\"\n",
    "                if not filtered_train_csv.exists():\n",
    "                    logger.warning(\"Filtered CSV not found at %s, using original train csv\", filtered_train_csv)\n",
    "                    filtered_train_csv = Path(paths[\"train_csv\"])\n",
    "                # rebuild dataloaders using filtered train set but same val/test\n",
    "                set_global_seed(config.seed, deterministic=True)\n",
    "                dls_filtered = make_data_loaders(str(filtered_train_csv), paths[\"val_csv\"], paths[\"test_csv\"], config)\n",
    "                train_loader = dls_filtered['train']\n",
    "                # keep train_full_loader as original full (for z_hat pred)\n",
    "                current_samples = len(train_loader.dataset)\n",
    "                logger.info(\"Iteration %d train_loader: %d samples (reduction %d, %.2f%%)\", i, current_samples,\n",
    "                            (original_total_samples - current_samples),\n",
    "                            ((original_total_samples - current_samples) / original_total_samples * 100.0))\n",
    "            else:\n",
    "                # initial train_loader is already set\n",
    "                current_samples = len(train_loader.dataset)\n",
    "\n",
    "            set_global_seed(config.seed, deterministic=True)\n",
    "            result = train_iteration(i, config, train_loader, val_loader, test_loader, train_full_loader, start_epoch=0)\n",
    "            indices = result['z_hat_indices']\n",
    "            z_hat = result['z_hat']\n",
    "\n",
    "            # update EMA\n",
    "            if z_ema_prev is None:\n",
    "                z_ema_prev = update_ema(None, z_hat, config.alpha)\n",
    "            else:\n",
    "                z_ema_prev = update_ema(z_ema_prev, z_hat, config.alpha)\n",
    "\n",
    "            # Save preds npz\n",
    "            npz_path = Path(config.exp_dir) / f\"iteration_{i}\" / \"preds_npz\" / f\"preds_iter_{i}.npz\"\n",
    "            os.makedirs(npz_path.parent, exist_ok=True)\n",
    "            save_npz(str(npz_path), indices=indices.astype('int32'), z_hat=z_hat.astype('float32'), z_ema=z_ema_prev.astype('float32'))\n",
    "\n",
    "            # Apply filter using your filter_by_ema - ensure it returns updated_df with filter_flag col and 'stats'\n",
    "            train_df = pd.read_csv(paths['train_csv'])\n",
    "            updated_df, stats = filter_by_ema(indices, z_ema_prev, original_train_df, config.min_keep_ratio)\n",
    "\n",
    "            # compute counts\n",
    "            kept_samples = int((updated_df['filter_flag'] == 'kept').sum())\n",
    "            removed_samples = int((updated_df['filter_flag'] == 'removed').sum())\n",
    "            total_samples = len(updated_df)\n",
    "\n",
    "            logger.info(\"Filtering results iteration %d: total=%d kept=%d removed=%d kept_ratio=%.4f\",\n",
    "                        i, total_samples, kept_samples, removed_samples, stats['_overall']['kept_ratio_total'])\n",
    "\n",
    "            # save preds csv (with filter_flag)\n",
    "            preds_csv_path = Path(config.exp_dir) / f\"iteration_{i}\" / \"preds\" / f\"preds_iter_{i}.csv\"\n",
    "            os.makedirs(preds_csv_path.parent, exist_ok=True)\n",
    "            updated_df.to_csv(preds_csv_path, index=False)\n",
    "\n",
    "            # create train_kept csv for next iter\n",
    "            train_kept_df = updated_df[updated_df['filter_flag'] == 'kept'].copy()\n",
    "            train_kept_csv_path = Path(config.exp_dir) / f\"iteration_{i}\" / f\"train_kept_{i}.csv\"\n",
    "            os.makedirs(train_kept_csv_path.parent, exist_ok=True)\n",
    "            train_kept_df.to_csv(train_kept_csv_path, index=False)\n",
    "\n",
    "            # Compose summary row with both orig/noisy acc (if available)\n",
    "            summary_row = {\n",
    "                'noise_ratio': noise_ratio,\n",
    "                'alpha': alpha,\n",
    "                'iteration': i,\n",
    "                'kept_ratio': stats['_overall']['kept_ratio_total'],\n",
    "                'val_acc_reported': result.get('best_val_acc', -1.0),\n",
    "                'test_acc_reported': result.get('test_acc', -1.0),\n",
    "                'val_acc_orig': result.get('val_acc_orig', None),\n",
    "                'test_acc_orig': result.get('test_acc_orig', None),\n",
    "                'val_acc_noisy': result.get('val_acc_noisy', None),\n",
    "                'test_acc_noisy': result.get('test_acc_noisy', None),\n",
    "                \n",
    "                # NEW — train_full metrics\n",
    "                'summary_train_full_acc_noisy': result.get('train_full_acc_noisy', None),\n",
    "                'summary_train_full_acc_orig': result.get('train_full_acc_orig', None),\n",
    "                \n",
    "                'samples_kept': kept_samples,\n",
    "                'samples_removed': removed_samples,\n",
    "                'samples_total': total_samples,\n",
    "                'training_samples_used': current_samples,\n",
    "                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            }\n",
    "            experiment_summary_rows.append(summary_row)\n",
    "\n",
    "            # Immediately persist experiment summary (append)\n",
    "            exp_df = pd.DataFrame(experiment_summary_rows)\n",
    "            exp_df.to_csv(experiment_summary_path, index=False)\n",
    "            logger.info(\"Saved experiment summary to %s\", experiment_summary_path)\n",
    "\n",
    "            # Update best_overall and early-stop across iterations\n",
    "            if result['best_val_acc'] > best_overall_val_acc:\n",
    "                best_overall_val_acc = result['best_val_acc']\n",
    "                iter_no_improve = 0\n",
    "            else:\n",
    "                iter_no_improve += 1\n",
    "                if iter_no_improve > config.patience_iter:\n",
    "                    logger.info(\"Stopping iterations for noise=%s alpha=%s due to no improvement across iterations.\", noise_ratio, alpha)\n",
    "                    break\n",
    "\n",
    "        # End iterations for this alpha\n",
    "        logger.info(\"Experiment completed for noise=%s alpha=%s. Summary saved to %s. Best overall val_acc=%.4f\",\n",
    "                    noise_ratio, alpha, experiment_summary_path, best_overall_val_acc)\n",
    "\n",
    "# End experiments\n",
    "logger.info(\"All experiments finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a56474-04b9-4fdb-a011-468519c3badd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (self_ensembling)",
   "language": "python",
   "name": "self_ensembling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
