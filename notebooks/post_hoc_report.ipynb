{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b19dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, re, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Project imports (reuse code)\n",
    "# ---------------------------\n",
    "def _find_project_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(8):\n",
    "        if (cur / \"src\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"Cannot find project root containing 'src' folder.\")\n",
    "\n",
    "PROJECT_ROOT = _find_project_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in os.sys.path:\n",
    "    os.sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "from src.config import TrainConfig\n",
    "from src.dataset_utils import make_data_loaders\n",
    "from src.model_utils import get_model\n",
    "from src.io_utils import load_checkpoint\n",
    "\n",
    "# ---------------------------\n",
    "# USER CONFIG\n",
    "# ---------------------------\n",
    "BASE_OUTPUT_DIR = Path(\"outputs/cifar10_iter_ema_voting_sweep\")  # <-- chỉnh theo folder outputs của bạn\n",
    "DATA_DIR = Path(\"data\")                                         # <-- chỉnh nếu khác\n",
    "\n",
    "# EXPERIMENT_PLAN: List[Dict[str, Any]] = [\n",
    "#     {\"noise_ratios\": [0.8], \"alphas\": [0.3, 0.2, 0.6, 0.8], \"modes\": [\"ema_hard\", \"vote_match_noisy\", \"vote_relabel\"]},\n",
    "#     {\"noise_ratios\": [0.6], \"alphas\": [0.3, 0.2, 0.6, 0.8], \"modes\": [\"ema_hard\", \"vote_match_noisy\", \"vote_relabel\"]},\n",
    "#     {\"noise_ratios\": [0.4], \"alphas\": [0.3, 0.2, 0.6, 0.8], \"modes\": [\"ema_hard\", \"vote_match_noisy\", \"vote_relabel\"]},\n",
    "#     {\"noise_ratios\": [0.2], \"alphas\": [0.3, 0.2, 0.6, 0.8], \"modes\": [\"ema_hard\", \"vote_match_noisy\", \"vote_relabel\"]},\n",
    "# ]\n",
    "\n",
    "EXPERIMENT_PLAN  = [\n",
    "    {\n",
    "        \"noise_ratios\": [0.8],\n",
    "        # \"alphas\": [0.3, 0.2, 0.6, 0.8],\n",
    "        \"alphas\": [0.3],\n",
    "        \"modes\": [\"vote_match_noisy\", \"vote_relabel\", \"ema_hard\"],\n",
    "    },\n",
    "    {\n",
    "        \"noise_ratios\": [0.6],\n",
    "        # \"alphas\": [0.6, 0.3, 0.2,  0.8],\n",
    "        \"alphas\": [0.6],\n",
    "        \"modes\": [\"vote_match_noisy\", \"vote_relabel\", \"ema_hard\"],\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"noise_ratios\": [0.4],\n",
    "        # \"alphas\": [0.8 ,0.3, 0.2, 0.6],        \n",
    "        \"alphas\": [0.8],        \n",
    "        \"modes\": [\"vote_match_noisy\", \"vote_relabel\", \"ema_hard\"],\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"noise_ratios\": [0.2],\n",
    "        # \"alphas\": [0.2 , 0.3 , 0.6, 0.8],\n",
    "        \"alphas\": [0.2],\n",
    "        \"modes\": [\"vote_match_noisy\", \"vote_relabel\", \"ema_hard\"],\n",
    "\n",
    "    },\n",
    "]\n",
    "\n",
    "# metric dùng để chọn \"best iteration\" trong report tổng hợp\n",
    "# ✅ nên set theo config training của bạn để tránh lệch logic\n",
    "# Ví dụ: nếu training early_stop_metric=\"val_acc_noisy\" thì để \"val_acc_noisy_posthoc\"\n",
    "BEST_PICK_KEY = \"val_acc_noisy_posthoc\"   # hoặc \"val_acc_orig_posthoc\", \"test_acc_orig_posthoc\", ...\n",
    "\n",
    "# ---------------------------\n",
    "# Utils\n",
    "# ---------------------------\n",
    "def _ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _format_seconds(sec: float) -> str:\n",
    "    sec = float(sec)\n",
    "    if sec < 60:\n",
    "        return f\"{sec:.1f}s\"\n",
    "    if sec < 3600:\n",
    "        return f\"{sec/60:.1f}m\"\n",
    "    return f\"{sec/3600:.2f}h\"\n",
    "\n",
    "def _iter_index_from_name(name: str) -> Optional[int]:\n",
    "    m = re.search(r\"iteration_(\\d+)\", name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def list_iteration_dirs(exp_dir: Path) -> List[Tuple[int, Path]]:\n",
    "    out = []\n",
    "    for p in exp_dir.glob(\"iteration_*\"):\n",
    "        if p.is_dir():\n",
    "            it = _iter_index_from_name(p.name)\n",
    "            if it is not None:\n",
    "                out.append((it, p))\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return out\n",
    "\n",
    "def resolve_checkpoint_for_iter(exp_dir: Path, it: int) -> Optional[Path]:\n",
    "    ck_best = exp_dir / f\"iteration_{it}\" / \"checkpoints\" / f\"model_iter{it}_best.pth\"\n",
    "    ck_last = exp_dir / f\"iteration_{it}\" / \"checkpoints\" / f\"model_iter{it}_last.pth\"\n",
    "    if ck_best.exists():\n",
    "        return ck_best\n",
    "    if ck_last.exists():\n",
    "        return ck_last\n",
    "    return None\n",
    "\n",
    "def save_confusion_matrix_png(cm: np.ndarray, out_png: Path, title: str = \"\") -> None:\n",
    "    _ensure_dir(out_png.parent)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_predictions(model: torch.nn.Module, loader, device: str, use_amp: bool) -> Dict[str, np.ndarray]:\n",
    "    model.eval()\n",
    "    y_pred, y_orig, y_noisy = [], [], []\n",
    "    has_orig, has_noisy = False, False\n",
    "\n",
    "    amp_ok = bool(use_amp and device.startswith(\"cuda\"))\n",
    "\n",
    "    t0 = time.time()\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(device, non_blocking=True)\n",
    "        if amp_ok:\n",
    "            with torch.autocast(device_type=\"cuda\", enabled=True):\n",
    "                logits = model(x)\n",
    "        else:\n",
    "            logits = model(x)\n",
    "\n",
    "        preds = logits.argmax(dim=1).cpu().numpy().astype(np.int32)\n",
    "        y_pred.append(preds)\n",
    "\n",
    "        if \"label_orig\" in batch:\n",
    "            y = batch[\"label_orig\"].cpu().numpy().astype(np.int32)\n",
    "            y_orig.append(y)\n",
    "            has_orig = True\n",
    "        if \"label_noisy\" in batch:\n",
    "            y = batch[\"label_noisy\"].cpu().numpy().astype(np.int32)\n",
    "            y_noisy.append(y)\n",
    "            has_noisy = True\n",
    "\n",
    "    t1 = time.time()\n",
    "    out = {\"y_pred\": np.concatenate(y_pred), \"infer_seconds\": float(t1 - t0)}\n",
    "    if has_orig:\n",
    "        out[\"y_true_orig\"] = np.concatenate(y_orig)\n",
    "    if has_noisy:\n",
    "        out[\"y_true_noisy\"] = np.concatenate(y_noisy)\n",
    "    return out\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, num_classes: int) -> Dict[str, Any]:\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    macro_f1 = float(f1_score(y_true, y_pred, average=\"macro\", labels=list(range(num_classes))))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    rep = classification_report(\n",
    "        y_true, y_pred,\n",
    "        labels=list(range(num_classes)),\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return {\"acc\": acc, \"macro_f1\": macro_f1, \"cm\": cm, \"report_dict\": rep}\n",
    "\n",
    "def find_run_dirs(base_dir: Path, noise: float, alpha: float, mode: str) -> List[Path]:\n",
    "    # base/noise_x/alpha_y/mode_z/(maybe suffix)\n",
    "    root = base_dir / f\"noise_{noise}\" / f\"alpha_{alpha}\" / f\"mode_{mode}\"\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    cands = []\n",
    "    if (root / \"experiment_summary.csv\").exists():\n",
    "        cands.append(root)\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir() and (p / \"experiment_summary.csv\").exists():\n",
    "            cands.append(p)\n",
    "    cands.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return cands\n",
    "\n",
    "def sum_train_time_seconds(exp_dir: Path) -> float:\n",
    "    total = 0.0\n",
    "    for it, it_dir in list_iteration_dirs(exp_dir):\n",
    "        m = it_dir / \"metrics\" / \"metrics_epoch.csv\"\n",
    "        if m.exists():\n",
    "            df = pd.read_csv(m)\n",
    "            if \"epoch_time\" in df.columns:\n",
    "                total += float(df[\"epoch_time\"].fillna(0).sum())\n",
    "    return float(total)\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate ALL iterations of 1 exp_dir\n",
    "# ---------------------------\n",
    "def evaluate_all_iterations(exp_dir: Path, noise: float, alpha: float, mode: str, data_dir: Path) -> Dict[str, Any]:\n",
    "    cfg = TrainConfig()\n",
    "    cfg.data_dir = str(data_dir)\n",
    "    cfg.noise_ratio = float(noise)\n",
    "    cfg.alpha = float(alpha)\n",
    "    cfg.filter_mode = str(mode)\n",
    "    cfg.exp_dir = str(exp_dir)\n",
    "\n",
    "    device = cfg.device\n",
    "    use_amp = bool(cfg.use_amp)\n",
    "\n",
    "    csv_dir = data_dir / \"csvs\" / f\"noise_{noise}\"\n",
    "    train_csv = str(csv_dir / \"train.csv\")\n",
    "    val_csv = str(csv_dir / \"val.csv\")\n",
    "    test_csv = str(csv_dir / \"test.csv\")\n",
    "    if not (Path(train_csv).exists() and Path(val_csv).exists() and Path(test_csv).exists()):\n",
    "        raise FileNotFoundError(f\"Missing CSVs at {csv_dir}\")\n",
    "\n",
    "    # loaders for inference (reuse make_data_loaders)\n",
    "    dls = make_data_loaders(\n",
    "        train_csv=train_csv,\n",
    "        val_csv=val_csv,\n",
    "        test_csv=test_csv,\n",
    "        config=cfg,\n",
    "        train_full_csv=train_csv,\n",
    "        train_label_col=\"label_noisy\",\n",
    "    )\n",
    "    val_loader = dls[\"val\"]\n",
    "    test_loader = dls[\"test\"]\n",
    "\n",
    "    it_dirs = list_iteration_dirs(exp_dir)\n",
    "    if len(it_dirs) == 0:\n",
    "        raise RuntimeError(f\"No iteration_* folders in {exp_dir}\")\n",
    "\n",
    "    report_dir = exp_dir / \"posthoc_report\"\n",
    "    _ensure_dir(report_dir)\n",
    "\n",
    "    rows = []\n",
    "    total_train_time = sum_train_time_seconds(exp_dir)\n",
    "\n",
    "    for it, _ in it_dirs:\n",
    "        ckpt = resolve_checkpoint_for_iter(exp_dir, it)\n",
    "        if ckpt is None:\n",
    "            rows.append({\n",
    "                \"iteration\": it,\n",
    "                \"status\": \"missing_checkpoint\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # load model\n",
    "        model = get_model(num_classes=cfg.num_classes, pretrained=True, device=device)\n",
    "        load_checkpoint(str(ckpt), model, optimizer=None, scheduler=None, map_location=device)\n",
    "\n",
    "        # infer\n",
    "        val_out = infer_predictions(model, val_loader, device=device, use_amp=use_amp)\n",
    "        test_out = infer_predictions(model, test_loader, device=device, use_amp=use_amp)\n",
    "\n",
    "        iter_out_dir = report_dir / f\"iter_{it}\"\n",
    "        _ensure_dir(iter_out_dir)\n",
    "\n",
    "        row = {\n",
    "            \"noise_ratio\": float(noise),\n",
    "            \"alpha\": float(alpha),\n",
    "            \"mode\": str(mode),\n",
    "            \"exp_dir\": str(exp_dir),\n",
    "            \"iteration\": int(it),\n",
    "            \"checkpoint\": str(ckpt),\n",
    "            \"infer_val_seconds\": float(val_out[\"infer_seconds\"]),\n",
    "            \"infer_test_seconds\": float(test_out[\"infer_seconds\"]),\n",
    "            \"train_time_seconds_sum\": float(total_train_time),\n",
    "            \"status\": \"ok\",\n",
    "        }\n",
    "\n",
    "        # metrics (orig)\n",
    "        if \"y_true_orig\" in val_out:\n",
    "            m = compute_metrics(val_out[\"y_true_orig\"], val_out[\"y_pred\"], cfg.num_classes)\n",
    "            row[\"val_acc_orig_posthoc\"] = m[\"acc\"]\n",
    "            row[\"val_macro_f1_orig_posthoc\"] = m[\"macro_f1\"]\n",
    "            pd.DataFrame(m[\"cm\"]).to_csv(iter_out_dir / \"val_confusion_matrix_orig.csv\", index=False)\n",
    "            save_confusion_matrix_png(m[\"cm\"], iter_out_dir / \"val_confusion_matrix_orig.png\",\n",
    "                                      title=f\"VAL(orig) it={it} noise={noise} alpha={alpha} mode={mode}\")\n",
    "            pd.DataFrame(m[\"report_dict\"]).to_csv(iter_out_dir / \"val_classification_report_orig.csv\")\n",
    "\n",
    "        if \"y_true_orig\" in test_out:\n",
    "            m = compute_metrics(test_out[\"y_true_orig\"], test_out[\"y_pred\"], cfg.num_classes)\n",
    "            row[\"test_acc_orig_posthoc\"] = m[\"acc\"]\n",
    "            row[\"test_macro_f1_orig_posthoc\"] = m[\"macro_f1\"]\n",
    "            pd.DataFrame(m[\"cm\"]).to_csv(iter_out_dir / \"test_confusion_matrix_orig.csv\", index=False)\n",
    "            save_confusion_matrix_png(m[\"cm\"], iter_out_dir / \"test_confusion_matrix_orig.png\",\n",
    "                                      title=f\"TEST(orig) it={it} noise={noise} alpha={alpha} mode={mode}\")\n",
    "            pd.DataFrame(m[\"report_dict\"]).to_csv(iter_out_dir / \"test_classification_report_orig.csv\")\n",
    "\n",
    "        # metrics (noisy) - hữu ích nếu bạn training/early-stop theo noisy metric\n",
    "        if \"y_true_noisy\" in val_out:\n",
    "            m = compute_metrics(val_out[\"y_true_noisy\"], val_out[\"y_pred\"], cfg.num_classes)\n",
    "            row[\"val_acc_noisy_posthoc\"] = m[\"acc\"]\n",
    "            row[\"val_macro_f1_noisy_posthoc\"] = m[\"macro_f1\"]\n",
    "\n",
    "        if \"y_true_noisy\" in test_out:\n",
    "            m = compute_metrics(test_out[\"y_true_noisy\"], test_out[\"y_pred\"], cfg.num_classes)\n",
    "            row[\"test_acc_noisy_posthoc\"] = m[\"acc\"]\n",
    "            row[\"test_macro_f1_noisy_posthoc\"] = m[\"macro_f1\"]\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    per_iter_df = pd.DataFrame(rows)\n",
    "    per_iter_df.to_csv(report_dir / \"per_iteration_metrics.csv\", index=False)\n",
    "\n",
    "    # pick best iteration for a compact summary (but you vẫn có full detail)\n",
    "    best_row = None\n",
    "    if BEST_PICK_KEY in per_iter_df.columns and per_iter_df[per_iter_df[\"status\"]==\"ok\"][BEST_PICK_KEY].notna().any():\n",
    "        best_row = per_iter_df[per_iter_df[\"status\"]==\"ok\"].sort_values(BEST_PICK_KEY, ascending=False).iloc[0]\n",
    "    else:\n",
    "        # fallback: max iteration\n",
    "        ok_df = per_iter_df[per_iter_df[\"status\"]==\"ok\"]\n",
    "        if len(ok_df) > 0:\n",
    "            best_row = ok_df.sort_values(\"iteration\", ascending=False).iloc[0]\n",
    "\n",
    "    if best_row is not None:\n",
    "        pd.DataFrame([best_row.to_dict()]).to_csv(report_dir / \"best_iteration_summary.csv\", index=False)\n",
    "\n",
    "    return {\n",
    "        \"exp_dir\": str(exp_dir),\n",
    "        \"noise_ratio\": float(noise),\n",
    "        \"alpha\": float(alpha),\n",
    "        \"mode\": str(mode),\n",
    "        \"n_iterations_found\": int(len(it_dirs)),\n",
    "        \"report_dir\": str(report_dir),\n",
    "        \"status\": \"ok\",\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN: sweep posthoc\n",
    "# ---------------------------\n",
    "all_runs = []\n",
    "t0 = time.time()\n",
    "\n",
    "for plan in EXPERIMENT_PLAN:\n",
    "    for noise in plan[\"noise_ratios\"]:\n",
    "        for alpha in plan[\"alphas\"]:\n",
    "            for mode in plan[\"modes\"]:\n",
    "                cands = find_run_dirs(BASE_OUTPUT_DIR, noise=noise, alpha=alpha, mode=mode)\n",
    "                if len(cands) == 0:\n",
    "                    print(f\"[MISS] noise={noise} alpha={alpha} mode={mode} -> no exp_dir\")\n",
    "                    all_runs.append({\n",
    "                        \"noise_ratio\": noise, \"alpha\": alpha, \"mode\": mode,\n",
    "                        \"status\": \"missing_exp_dir\", \"exp_dir\": None\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                exp_dir = cands[0]\n",
    "                print(f\"\\n[POSTHOC] noise={noise} alpha={alpha} mode={mode}\")\n",
    "                print(\"         exp_dir =\", exp_dir)\n",
    "\n",
    "                try:\n",
    "                    out = evaluate_all_iterations(exp_dir, noise=noise, alpha=alpha, mode=mode, data_dir=DATA_DIR)\n",
    "                    all_runs.append(out)\n",
    "                except Exception as e:\n",
    "                    all_runs.append({\n",
    "                        \"noise_ratio\": noise, \"alpha\": alpha, \"mode\": mode,\n",
    "                        \"status\": f\"error: {e}\", \"exp_dir\": str(exp_dir)\n",
    "                    })\n",
    "\n",
    "agg_dir = BASE_OUTPUT_DIR / \"_POSTHOC_AGGREGATE\"\n",
    "_ensure_dir(agg_dir)\n",
    "df_runs = pd.DataFrame(all_runs)\n",
    "df_runs.to_csv(agg_dir / \"posthoc_run_status.csv\", index=False)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"\\nDONE posthoc.\")\n",
    "print(\"Saved:\", agg_dir / \"posthoc_run_status.csv\")\n",
    "print(\"Elapsed:\", _format_seconds(t1 - t0))\n",
    "\n",
    "display(df_runs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (self_ensembling)",
   "language": "python",
   "name": "self_ensembling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
